---
title: "Jupyter Notebookã§å§‹ã‚ã‚‹ç«¶é¦¬AIé–‹ç™ºç’°å¢ƒ"
date: 2025-08-07T12:00:00+09:00
description: "Jupyter Notebookã‚’ä½¿ã£ãŸç«¶é¦¬AIé–‹ç™ºç’°å¢ƒã®æ§‹ç¯‰ã‹ã‚‰å®Ÿè£…ã¾ã§ã€‚ãƒ‡ãƒ¼ã‚¿åˆ†æã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã€å¯è¦–åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’å®Ÿä¾‹ã¨ã¨ã‚‚ã«è©³ã—ãè§£èª¬ã€‚"
categories: ["Tools", "AI", "Data Science"]
tags: ["Jupyter Notebook", "ç«¶é¦¬AI", "æ©Ÿæ¢°å­¦ç¿’", "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹", "Python", "pandas", "scikit-learn", "TensorFlow", "ãƒ‡ãƒ¼ã‚¿åˆ†æ", "å¯è¦–åŒ–"]
slug: "jupyter-horse-ai-development"
---

## ã¯ã˜ã‚ã«

ç«¶é¦¬AIé–‹ç™ºã«ãŠã„ã¦ã€Jupyter Notebookã¯ç ”ç©¶ã‹ã‚‰å®Ÿè£…ã¾ã§ä¸€è²«ã—ã¦ä½¿ç”¨ã§ãã‚‹å¼·åŠ›ãªé–‹ç™ºç’°å¢ƒã§ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€Jupyter Notebookã‚’ä½¿ç”¨ã—ãŸç«¶é¦¬AIé–‹ç™ºã®å®Œå…¨ã‚¬ã‚¤ãƒ‰ã‚’æä¾›ã—ã€ç’°å¢ƒæ§‹ç¯‰ã‹ã‚‰é«˜åº¦ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã¾ã§ã€å®Ÿè·µçš„ãªçŸ¥è­˜ã‚’ä½“ç³»çš„ã«è§£èª¬ã—ã¾ã™ã€‚

## 1. Jupyter Notebooké–‹ç™ºç’°å¢ƒã®ç†å¿µ

### 1.1 ãªãœJupyter Notebookãªã®ã‹

Jupyter Notebookã¯ä»¥ä¸‹ã®ç†ç”±ã§ç«¶é¦¬AIé–‹ç™ºã«æœ€é©ã§ã™ï¼š

- **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªé–‹ç™º**: ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œçµæœã‚’å³åº§ã«ç¢ºèª
- **ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–**: ã‚°ãƒ©ãƒ•ã‚„ãƒãƒ£ãƒ¼ãƒˆã‚’ç›´æ¥è¡¨ç¤º
- **å®Ÿé¨“ç®¡ç†**: åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã¨çµæœã‚’ä¸€å…ƒç®¡ç†
- **å…±æœ‰ã—ã‚„ã™ã•**: ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®çŸ¥è¦‹å…±æœ‰
- **ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°**: è¿…é€Ÿãªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã¨æ¤œè¨¼

### 1.2 ç«¶é¦¬AIé–‹ç™ºã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```
ãƒ‡ãƒ¼ã‚¿åé›† â†’ å‰å‡¦ç† â†’ æ¢ç´¢çš„åˆ†æ â†’ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° â†’ ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ â†’ è©•ä¾¡ãƒ»æ”¹å–„
```

## 2. é–‹ç™ºç’°å¢ƒã®æ§‹ç¯‰

### 2.1 Anacondaç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Anacondaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
# https://www.anaconda.com/products/distribution

# ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
conda create -n horse-ai python=3.9
conda activate horse-ai

# Jupyter Notebookã¨ã‚¨ã‚¯ã‚¹ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨­å®š
conda install jupyter jupyterlab
conda install nb_conda_kernels
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
```

### 2.2 å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»åˆ†æãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install pandas numpy scipy scikit-learn

# æ©Ÿæ¢°å­¦ç¿’ãƒ»æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
pip install tensorflow pytorch xgboost lightgbm catboost

# ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install matplotlib seaborn plotly bokeh altair

# çµ±è¨ˆãƒ»æ•°å€¤è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install statsmodels sympy

# Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»APIé€£æº
pip install requests beautifulsoup4 selenium

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
pip install sqlalchemy pymongo psycopg2-binary

# ç‰¹æ®Šç”¨é€”ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install shap lime optuna hyperopt

# Jupyteræ‹¡å¼µæ©Ÿèƒ½
pip install ipywidgets tqdm jupyterlab-git
```

### 2.3 Jupyter Labè¨­å®šã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```python
# ~/.jupyter/jupyter_notebook_config.py
c = get_config()

# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®è¨­å®š
c.NotebookApp.notebook_dir = '/path/to/your/horse-ai-projects'
c.NotebookApp.open_browser = True
c.NotebookApp.port = 8888

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
c.NotebookApp.token = ''
c.NotebookApp.password = ''

# è‡ªå‹•ä¿å­˜é–“éš”ï¼ˆç§’ï¼‰
c.FileContentsManager.autosave_interval = 300

# ã‚«ã‚¹ã‚¿ãƒ CSSã§ãƒ€ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒé©ç”¨
c.NotebookApp.extra_static_paths = ["/path/to/custom/css"]
```

## 3. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 3.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
horse-ai-project/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_development.ipynb
â”‚   â”œâ”€â”€ 04_model_evaluation.ipynb
â”‚   â””â”€â”€ 05_prediction_system.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ external/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained_models/
â”‚   â””â”€â”€ model_configs/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ analysis_reports/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### 3.2 Jupyter Notebookå®Ÿè¡Œç’°å¢ƒã®åˆæœŸè¨­å®š

```python
# ã‚»ãƒ«1: ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿
import sys
sys.path.append('../src')

# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Jupyterè¨­å®š
%matplotlib inline
%config InlineBackend.figure_format = 'retina'
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# pandasè¡¨ç¤ºè¨­å®š
pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', '{:.3f}'.format)

# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
from tqdm.notebook import tqdm
tqdm.pandas()

print("ç’°å¢ƒè¨­å®šå®Œäº†")
```

## 4. ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã¨å‰å‡¦ç†

### 4.1 ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬çš„ãªæ¢ç´¢

```python
# ã‚»ãƒ«2: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
def load_horse_data():
    """ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    
    # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
    races_df = pd.read_csv('../data/raw/races.csv', parse_dates=['race_date'])
    
    # é¦¬ãƒ‡ãƒ¼ã‚¿
    horses_df = pd.read_csv('../data/raw/horses.csv')
    
    # æˆç¸¾ãƒ‡ãƒ¼ã‚¿
    results_df = pd.read_csv('../data/raw/race_results.csv', parse_dates=['race_date'])
    
    # é¨æ‰‹ãƒ‡ãƒ¼ã‚¿
    jockeys_df = pd.read_csv('../data/raw/jockeys.csv')
    
    # èª¿æ•™å¸«ãƒ‡ãƒ¼ã‚¿
    trainers_df = pd.read_csv('../data/raw/trainers.csv')
    
    print(f"ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿: {races_df.shape}")
    print(f"é¦¬ãƒ‡ãƒ¼ã‚¿: {horses_df.shape}")
    print(f"æˆç¸¾ãƒ‡ãƒ¼ã‚¿: {results_df.shape}")
    print(f"é¨æ‰‹ãƒ‡ãƒ¼ã‚¿: {jockeys_df.shape}")
    print(f"èª¿æ•™å¸«ãƒ‡ãƒ¼ã‚¿: {trainers_df.shape}")
    
    return races_df, horses_df, results_df, jockeys_df, trainers_df

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Ÿè¡Œ
races_df, horses_df, results_df, jockeys_df, trainers_df = load_horse_data()

# åŸºæœ¬æƒ…å ±ã®ç¢ºèª
display(races_df.head())
display(races_df.info())
display(races_df.describe())
```

### 4.2 ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç¢ºèªã¨å¯è¦–åŒ–

```python
# ã‚»ãƒ«3: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
def analyze_data_quality(df, name):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªã®åˆ†æ"""
    print(f"\n=== {name} ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ ===")
    
    # æ¬ æå€¤ã®ç¢ºèª
    missing_data = df.isnull().sum()
    missing_percent = (missing_data / len(df)) * 100
    
    quality_df = pd.DataFrame({
        'Missing Count': missing_data,
        'Missing Percentage': missing_percent
    }).sort_values('Missing Percentage', ascending=False)
    
    # æ¬ æå€¤ã®å¯è¦–åŒ–
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # æ¬ æå€¤ã®æ£’ã‚°ãƒ©ãƒ•
    quality_df[quality_df['Missing Percentage'] > 0]['Missing Percentage'].plot(
        kind='bar', ax=ax1, color='coral'
    )
    ax1.set_title(f'{name} - Missing Data Percentage')
    ax1.set_ylabel('Percentage (%)')
    plt.xticks(rotation=45)
    
    # æ¬ æå€¤ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    sns.heatmap(df.isnull(), yticklabels=False, cbar=True, ax=ax2, cmap='viridis')
    ax2.set_title(f'{name} - Missing Data Heatmap')
    
    plt.tight_layout()
    plt.show()
    
    return quality_df[quality_df['Missing Percentage'] > 0]

# å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å“è³ªç¢ºèª
race_quality = analyze_data_quality(races_df, "Races")
results_quality = analyze_data_quality(results_df, "Results")
horse_quality = analyze_data_quality(horses_df, "Horses")
```

### 4.3 æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰

```python
# ã‚»ãƒ«4: æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ
def exploratory_data_analysis():
    """åŒ…æ‹¬çš„ãªEDA"""
    
    # ãƒ¬ãƒ¼ã‚¹åˆ†å¸ƒã®åˆ†æ
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. ç«¶é¦¬å ´åˆ¥ãƒ¬ãƒ¼ã‚¹æ•°
    track_counts = races_df['track'].value_counts()
    track_counts.plot(kind='bar', ax=axes[0,0], color='skyblue')
    axes[0,0].set_title('ãƒ¬ãƒ¼ã‚¹æ•°ï¼ˆç«¶é¦¬å ´åˆ¥ï¼‰')
    axes[0,0].set_ylabel('ãƒ¬ãƒ¼ã‚¹æ•°')
    
    # 2. è·é›¢åˆ†å¸ƒ
    races_df['distance'].hist(bins=30, ax=axes[0,1], color='lightgreen', alpha=0.7)
    axes[0,1].set_title('è·é›¢åˆ†å¸ƒ')
    axes[0,1].set_xlabel('è·é›¢ (m)')
    axes[0,1].set_ylabel('é »åº¦')
    
    # 3. ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
    grade_counts = races_df['grade'].value_counts()
    axes[0,2].pie(grade_counts.values, labels=grade_counts.index, autopct='%1.1f%%')
    axes[0,2].set_title('ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ')
    
    # 4. ç€é †åˆ†å¸ƒ
    finish_counts = results_df['finish_position'].value_counts().sort_index()
    finish_counts.head(10).plot(kind='bar', ax=axes[1,0], color='orange')
    axes[1,0].set_title('ç€é †åˆ†å¸ƒï¼ˆä¸Šä½10ä½ï¼‰')
    axes[1,0].set_xlabel('ç€é †')
    axes[1,0].set_ylabel('é »åº¦')
    
    # 5. é¦¬é½¢åˆ†å¸ƒ
    results_df['horse_age'].hist(bins=15, ax=axes[1,1], color='pink', alpha=0.7)
    axes[1,1].set_title('é¦¬é½¢åˆ†å¸ƒ')
    axes[1,1].set_xlabel('å¹´é½¢')
    axes[1,1].set_ylabel('é »åº¦')
    
    # 6. ã‚ªãƒƒã‚ºåˆ†å¸ƒï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    results_df['odds'].replace(0, np.nan, inplace=True)
    log_odds = np.log(results_df['odds'].dropna())
    log_odds.hist(bins=50, ax=axes[1,2], color='purple', alpha=0.7)
    axes[1,2].set_title('ã‚ªãƒƒã‚ºåˆ†å¸ƒï¼ˆå¯¾æ•°ï¼‰')
    axes[1,2].set_xlabel('log(ã‚ªãƒƒã‚º)')
    axes[1,2].set_ylabel('é »åº¦')
    
    plt.tight_layout()
    plt.show()
    
    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    print("\n=== åŸºæœ¬çµ±è¨ˆæƒ…å ± ===")
    display(results_df[['horse_age', 'weight', 'odds', 'finish_position']].describe())

# EDAå®Ÿè¡Œ
exploratory_data_analysis()
```

### 4.4 ç›¸é–¢åˆ†æã¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹

```python
# ã‚»ãƒ«5: ç›¸é–¢åˆ†æ
def correlation_analysis():
    """ç‰¹å¾´é‡é–“ã®ç›¸é–¢åˆ†æ"""
    
    # æ•°å€¤ç‰¹å¾´é‡ã®æŠ½å‡º
    numeric_features = [
        'horse_age', 'weight', 'odds', 'popularity', 
        'gate_number', 'horse_number', 'distance'
    ]
    
    # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ã®çµåˆ
    analysis_df = results_df.merge(
        races_df[['race_id', 'distance', 'track_condition']], 
        on='race_id'
    )
    
    # ç›¸é–¢è¡Œåˆ—ã®è¨ˆç®—
    correlation_matrix = analysis_df[numeric_features].corr()
    
    # ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    plt.figure(figsize=(12, 10))
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(
        correlation_matrix, 
        mask=mask, 
        annot=True, 
        cmap='RdBu_r', 
        center=0,
        square=True,
        linewidths=0.5,
        cbar_kws={"shrink": 0.8}
    )
    plt.title('ç‰¹å¾´é‡ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—')
    plt.tight_layout()
    plt.show()
    
    # ç€é †ã¨ã®ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡
    finish_correlation = analysis_df[numeric_features + ['finish_position']].corr()['finish_position'].abs().sort_values(ascending=False)
    
    print("\n=== ç€é †ã¨ã®ç›¸é–¢ï¼ˆçµ¶å¯¾å€¤ï¼‰ ===")
    display(finish_correlation.drop('finish_position'))
    
    return correlation_matrix

correlation_matrix = correlation_analysis()
```

## 5. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

### 5.1 åŸºæœ¬çš„ãªç‰¹å¾´é‡ä½œæˆ

```python
# ã‚»ãƒ«6: åŸºæœ¬ç‰¹å¾´é‡ã®ä½œæˆ
def create_basic_features(df):
    """åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’ä½œæˆ"""
    
    df = df.copy()
    
    # æ™‚é–“ç³»ç‰¹å¾´é‡
    df['race_year'] = df['race_date'].dt.year
    df['race_month'] = df['race_date'].dt.month
    df['race_day_of_week'] = df['race_date'].dt.dayofweek
    df['race_quarter'] = df['race_date'].dt.quarter
    
    # è·é›¢ç³»ç‰¹å¾´é‡
    df['distance_category'] = pd.cut(
        df['distance'], 
        bins=[0, 1200, 1600, 2000, 2400, 5000],
        labels=['çŸ­è·é›¢', 'ãƒã‚¤ãƒ«', 'ä¸­è·é›¢', 'é•·è·é›¢', 'è¶…é•·è·é›¢']
    )
    
    # ä½“é‡ç³»ç‰¹å¾´é‡
    df['weight_category'] = pd.cut(
        df['weight'],
        bins=[0, 450, 480, 520, 600],
        labels=['è»½é‡', 'æ™®é€š', 'é‡é‡', 'è¶…é‡é‡']
    )
    
    # ã‚ªãƒƒã‚ºç³»ç‰¹å¾´é‡
    df['odds_log'] = np.log1p(df['odds'])
    df['odds_category'] = pd.cut(
        df['odds'],
        bins=[0, 3, 10, 50, 999],
        labels=['æœ¬å‘½', 'å¯¾æŠ—', 'ç©´', 'å¤§ç©´']
    )
    
    # äººæ°—ç³»ç‰¹å¾´é‡
    df['popularity_category'] = pd.cut(
        df['popularity'],
        bins=[0, 3, 6, 12, 999],
        labels=['ä¸Šä½äººæ°—', 'ä¸­ä½äººæ°—', 'ä¸‹ä½äººæ°—', 'è¶…ä¸äººæ°—']
    )
    
    # ã‚²ãƒ¼ãƒˆä½ç½®ç‰¹å¾´é‡
    df['gate_position'] = df['gate_number'].apply(
        lambda x: 'å†…æ ' if x <= 4 else 'å¤–æ ' if x >= 13 else 'ä¸­æ '
    )
    
    print(f"ç‰¹å¾´é‡è¿½åŠ å®Œäº†ã€‚æ–°ã—ã„ç‰¹å¾´é‡æ•°: {len([col for col in df.columns if col not in results_df.columns])}")
    
    return df

# ç‰¹å¾´é‡ä½œæˆã®å®Ÿè¡Œ
enhanced_df = create_basic_features(
    results_df.merge(races_df[['race_id', 'distance', 'track', 'race_date']], on='race_id')
)

display(enhanced_df.head())
```

### 5.2 é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

```python
# ã‚»ãƒ«7: é«˜åº¦ãªç‰¹å¾´é‡ä½œæˆ
def create_advanced_features(df):
    """é«˜åº¦ãªç‰¹å¾´é‡ã‚’ä½œæˆ"""
    
    df = df.copy()
    
    # éå»æˆç¸¾ç‰¹å¾´é‡ã®ä½œæˆ
    df = df.sort_values(['horse_id', 'race_date'])
    
    # ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆé‡ï¼ˆéå»5æˆ¦ï¼‰
    rolling_features = df.groupby('horse_id').rolling(5, min_periods=1)
    
    df['avg_finish_position_5'] = rolling_features['finish_position'].mean().values
    df['avg_odds_5'] = rolling_features['odds'].mean().values
    df['win_rate_5'] = (rolling_features['finish_position'].apply(lambda x: (x == 1).sum()) / 5).values
    df['place_rate_5'] = (rolling_features['finish_position'].apply(lambda x: (x <= 3).sum()) / 5).values
    
    # é¨æ‰‹ãƒ»èª¿æ•™å¸«çµ±è¨ˆ
    jockey_stats = df.groupby('jockey_id').agg({
        'finish_position': ['mean', 'std'],
        'odds': 'mean'
    }).round(3)
    jockey_stats.columns = ['jockey_avg_finish', 'jockey_std_finish', 'jockey_avg_odds']
    
    trainer_stats = df.groupby('trainer_id').agg({
        'finish_position': ['mean', 'std'],
        'odds': 'mean'
    }).round(3)
    trainer_stats.columns = ['trainer_avg_finish', 'trainer_std_finish', 'trainer_avg_odds']
    
    # çµ±è¨ˆé‡ã‚’ãƒãƒ¼ã‚¸
    df = df.merge(jockey_stats, left_on='jockey_id', right_index=True, how='left')
    df = df.merge(trainer_stats, left_on='trainer_id', right_index=True, how='left')
    
    # ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³é©æ€§
    track_condition_performance = df.groupby(['horse_id', 'track_condition'])['finish_position'].mean()
    df['track_condition_aptitude'] = df.apply(
        lambda x: track_condition_performance.get((x['horse_id'], x['track_condition']), x['finish_position']), 
        axis=1
    )
    
    # è·é›¢é©æ€§
    distance_performance = df.groupby('horse_id')['distance'].apply(
        lambda x: x.iloc[-1] if len(x) > 0 else 1600  # æœ€å¾Œã«èµ°ã£ãŸè·é›¢
    )
    df['distance_aptitude'] = df.apply(
        lambda x: abs(x['distance'] - distance_performance.get(x['horse_id'], 1600)), 
        axis=1
    )
    
    # å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³
    seasonal_performance = df.groupby(['horse_id', 'race_month'])['finish_position'].mean()
    df['seasonal_aptitude'] = df.apply(
        lambda x: seasonal_performance.get((x['horse_id'], x['race_month']), x['finish_position']),
        axis=1
    )
    
    print("é«˜åº¦ãªç‰¹å¾´é‡ä½œæˆå®Œäº†")
    
    return df

# é«˜åº¦ãªç‰¹å¾´é‡ä½œæˆã®å®Ÿè¡Œ
advanced_df = create_advanced_features(enhanced_df)
print(f"ç·ç‰¹å¾´é‡æ•°: {advanced_df.shape[1]}")
display(advanced_df[['horse_id', 'avg_finish_position_5', 'win_rate_5', 'jockey_avg_finish', 'distance_aptitude']].head())
```

### 5.3 ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ

```python
# ã‚»ãƒ«8: ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder

def analyze_feature_importance(df):
    """ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æ"""
    
    # æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®åˆ†é›¢
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä»¥å¤–ã®ç‰¹å¾´é‡
    feature_columns = [col for col in numeric_features if col not in ['finish_position', 'race_id', 'horse_id']]
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    df_encoded = df.copy()
    label_encoders = {}
    
    for col in categorical_features:
        if col in df_encoded.columns:
            le = LabelEncoder()
            df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col].astype(str))
            feature_columns.append(col + '_encoded')
            label_encoders[col] = le
    
    # æ¬ æå€¤ã®å‡¦ç†
    df_clean = df_encoded[feature_columns + ['finish_position']].dropna()
    
    # Random Forestã§é‡è¦åº¦è¨ˆç®—
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    X = df_clean[feature_columns]
    y = df_clean['finish_position']
    
    rf.fit(X, y)
    
    # é‡è¦åº¦ã‚’DataFrameã«ã¾ã¨ã‚
    importance_df = pd.DataFrame({
        'feature': feature_columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # ä¸Šä½20ç‰¹å¾´é‡ã®å¯è¦–åŒ–
    plt.figure(figsize=(12, 8))
    top_features = importance_df.head(20)
    sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')
    plt.title('ç‰¹å¾´é‡é‡è¦åº¦ (Top 20)')
    plt.xlabel('é‡è¦åº¦')
    plt.tight_layout()
    plt.show()
    
    print("\n=== ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½10ï¼‰ ===")
    display(importance_df.head(10))
    
    return importance_df, rf

importance_df, feature_model = analyze_feature_importance(advanced_df)
```

## 6. æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…

### 6.1 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰

```python
# ã‚»ãƒ«9: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import StandardScaler

def create_baseline_models(df):
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆãƒ»è©•ä¾¡"""
    
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    feature_columns = importance_df['feature'].head(15).tolist()  # ä¸Šä½15ç‰¹å¾´é‡ã‚’ä½¿ç”¨
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    df_model = df.copy()
    categorical_cols = df_model.select_dtypes(include=['object', 'category']).columns
    
    for col in categorical_cols:
        if col in df_model.columns:
            le = LabelEncoder()
            df_model[col + '_encoded'] = le.fit_transform(df_model[col].astype(str))
            if col + '_encoded' not in feature_columns and col in feature_columns:
                feature_columns = [c if c != col else col + '_encoded' for c in feature_columns]
    
    # æ¬ æå€¤ã®å‡¦ç†
    df_clean = df_model[feature_columns + ['finish_position']].dropna()
    
    X = df_clean[feature_columns]
    y = df_clean['finish_position']
    
    # å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=pd.cut(y, bins=5, labels=False)
    )
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'Lasso': Lasso(alpha=0.1),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
    }
    
    results = {}
    
    print("=== ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ===")
    
    for name, model in models.items():
        if name in ['Random Forest']:
            # Treeç³»ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
        else:
            # ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå¿…è¦
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
        
        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        results[name] = {
            'MSE': mse,
            'MAE': mae,
            'R2': r2,
            'RMSE': np.sqrt(mse)
        }
        
        print(f"\n{name}:")
        print(f"  MSE: {mse:.4f}")
        print(f"  MAE: {mae:.4f}")
        print(f"  RÂ²: {r2:.4f}")
        print(f"  RMSE: {np.sqrt(mse):.4f}")
    
    # çµæœã®å¯è¦–åŒ–
    results_df = pd.DataFrame(results).T
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    for i, metric in enumerate(['MSE', 'MAE', 'R2', 'RMSE']):
        ax = axes[i//2, i%2]
        results_df[metric].plot(kind='bar', ax=ax, color='skyblue')
        ax.set_title(f'{metric} Comparison')
        ax.set_ylabel(metric)
        plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    return results, models, scaler, X_train, X_test, y_train, y_test

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ
baseline_results, models, scaler, X_train, X_test, y_train, y_test = create_baseline_models(advanced_df)
```

### 6.2 é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«

```python
# ã‚»ãƒ«10: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import VotingRegressor, StackingRegressor

def create_ensemble_models(X_train, X_test, y_train, y_test):
    """é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    
    print("=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===")
    
    # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
    rf_model = RandomForestRegressor(
        n_estimators=200,
        max_depth=10,
        min_samples_split=5,
        random_state=42
    )
    
    xgb_model = xgb.XGBRegressor(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42
    )
    
    lgb_model = lgb.LGBMRegressor(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbosity=-1
    )
    
    # Votingã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    voting_ensemble = VotingRegressor([
        ('rf', rf_model),
        ('xgb', xgb_model),
        ('lgb', lgb_model)
    ])
    
    # Stackingã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    stacking_ensemble = StackingRegressor(
        estimators=[
            ('rf', rf_model),
            ('xgb', xgb_model),
            ('lgb', lgb_model)
        ],
        final_estimator=Ridge(alpha=1.0),
        cv=5
    )
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡
    ensemble_models = {
        'Random Forest': rf_model,
        'XGBoost': xgb_model,
        'LightGBM': lgb_model,
        'Voting Ensemble': voting_ensemble,
        'Stacking Ensemble': stacking_ensemble
    }
    
    ensemble_results = {}
    
    for name, model in ensemble_models.items():
        print(f"\nå­¦ç¿’ä¸­: {name}")
        
        # å­¦ç¿’
        if name == 'Stacking Ensemble':
            model.fit(X_train, y_train)
        else:
            model.fit(X_train, y_train)
        
        # äºˆæ¸¬
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # è©•ä¾¡
        train_mse = mean_squared_error(y_train, y_pred_train)
        test_mse = mean_squared_error(y_test, y_pred_test)
        train_mae = mean_absolute_error(y_train, y_pred_train)
        test_mae = mean_absolute_error(y_test, y_pred_test)
        
        ensemble_results[name] = {
            'Train MSE': train_mse,
            'Test MSE': test_mse,
            'Train MAE': train_mae,
            'Test MAE': test_mae,
            'Overfitting': test_mse - train_mse
        }
        
        print(f"  Train MSE: {train_mse:.4f}")
        print(f"  Test MSE: {test_mse:.4f}")
        print(f"  Test MAE: {test_mae:.4f}")
    
    # çµæœã®å¯è¦–åŒ–
    results_df = pd.DataFrame(ensemble_results).T
    
    plt.figure(figsize=(15, 8))
    
    plt.subplot(2, 2, 1)
    results_df[['Train MSE', 'Test MSE']].plot(kind='bar', ax=plt.gca())
    plt.title('MSE Comparison (Train vs Test)')
    plt.xticks(rotation=45)
    
    plt.subplot(2, 2, 2)
    results_df['Overfitting'].plot(kind='bar', ax=plt.gca(), color='red', alpha=0.7)
    plt.title('Overfitting (Test MSE - Train MSE)')
    plt.xticks(rotation=45)
    
    plt.subplot(2, 2, 3)
    results_df['Test MAE'].plot(kind='bar', ax=plt.gca(), color='green')
    plt.title('Test MAE')
    plt.xticks(rotation=45)
    
    plt.subplot(2, 2, 4)
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§ã®äºˆæ¸¬ vs å®Ÿå€¤
    best_model_name = results_df['Test MSE'].idxmin()
    best_model = ensemble_models[best_model_name]
    y_pred_best = best_model.predict(X_test)
    
    plt.scatter(y_test, y_pred_best, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Actual')
    plt.ylabel('Predicted')
    plt.title(f'Best Model: {best_model_name}')
    
    plt.tight_layout()
    plt.show()
    
    return ensemble_results, ensemble_models

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ
ensemble_results, ensemble_models = create_ensemble_models(X_train, X_test, y_train, y_test)
```

### 6.3 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

```python
# ã‚»ãƒ«11: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
import optuna
from sklearn.model_selection import cross_val_score

def optimize_hyperparameters(X_train, y_train, model_type='xgboost'):
    """Optunaã‚’ä½¿ç”¨ã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
    
    def objective(trial):
        if model_type == 'xgboost':
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
            }
            
            model = xgb.XGBRegressor(**params, random_state=42)
            
        elif model_type == 'lightgbm':
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
            }
            
            model = lgb.LGBMRegressor(**params, random_state=42, verbosity=-1)
        
        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        cv_scores = cross_val_score(
            model, X_train, y_train, 
            cv=5, scoring='neg_mean_squared_error', n_jobs=-1
        )
        
        return -cv_scores.mean()  # Optunaã¯æœ€å°åŒ–ã™ã‚‹ãŸã‚è² å·ã‚’ã¤ã‘ã‚‹
    
    print(f"=== {model_type.upper()} ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– ===")
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=100, show_progress_bar=True)
    
    print(f"\nBest parameters: {study.best_params}")
    print(f"Best CV score: {study.best_value:.4f}")
    
    # æœ€é©åŒ–çµæœã®å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æœ€é©åŒ–å±¥æ­´
    optuna.visualization.matplotlib.plot_optimization_history(study, ax=axes[0,0])
    axes[0,0].set_title('Optimization History')
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦
    optuna.visualization.matplotlib.plot_param_importances(study, ax=axes[0,1])
    axes[0,1].set_title('Parameter Importances')
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–¢ä¿‚æ€§ï¼ˆä¸Šä½2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
    important_params = list(study.best_params.keys())[:2]
    if len(important_params) >= 2:
        optuna.visualization.matplotlib.plot_contour(study, params=important_params, ax=axes[1,0])
        axes[1,0].set_title(f'Contour Plot: {important_params[0]} vs {important_params[1]}')
    
    # è©¦è¡Œå€¤ã®åˆ†å¸ƒ
    axes[1,1].hist([trial.value for trial in study.trials], bins=20, alpha=0.7)
    axes[1,1].set_xlabel('Objective Value (MSE)')
    axes[1,1].set_ylabel('Frequency')
    axes[1,1].set_title('Trial Values Distribution')
    
    plt.tight_layout()
    plt.show()
    
    # æœ€é©ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    if model_type == 'xgboost':
        best_model = xgb.XGBRegressor(**study.best_params, random_state=42)
    elif model_type == 'lightgbm':
        best_model = lgb.LGBMRegressor(**study.best_params, random_state=42, verbosity=-1)
    
    best_model.fit(X_train, y_train)
    
    return best_model, study.best_params

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã®å®Ÿè¡Œ
print("XGBoostæœ€é©åŒ–ä¸­...")
optimized_xgb, best_xgb_params = optimize_hyperparameters(X_train, y_train, 'xgboost')

print("\nLightGBMæœ€é©åŒ–ä¸­...")
optimized_lgb, best_lgb_params = optimize_hyperparameters(X_train, y_train, 'lightgbm')
```

## 7. ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã¨å¯è¦–åŒ–

### 7.1 SHAP ã«ã‚ˆã‚‹èª¬æ˜å¯èƒ½AI

```python
# ã‚»ãƒ«12: SHAPåˆ†æ
import shap

def analyze_model_with_shap(model, X_test, feature_names):
    """SHAPã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«è§£é‡ˆ"""
    
    print("=== SHAPåˆ†æ ===")
    
    # SHAPã®Explainerã‚’ä½œæˆ
    if hasattr(model, 'predict_proba'):
        explainer = shap.TreeExplainer(model)
    else:
        explainer = shap.Explainer(model, X_train[:100])  # ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨
    
    # SHAPå€¤ã‚’è¨ˆç®—ï¼ˆè¨ˆç®—æ™‚é–“ã‚’è€ƒæ…®ã—ã¦ä¸€éƒ¨ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰
    shap_values = explainer.shap_values(X_test[:500])
    
    # Summary Plot
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X_test[:500], feature_names=feature_names, show=False)
    plt.title('SHAP Summary Plot')
    plt.tight_layout()
    plt.show()
    
    # Feature Importance
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_test[:500], feature_names=feature_names, plot_type="bar", show=False)
    plt.title('SHAP Feature Importance')
    plt.tight_layout()
    plt.show()
    
    # å€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜ï¼ˆæœ€åˆã®5ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    for i in range(min(3, len(X_test))):
        plt.figure(figsize=(10, 6))
        shap.waterfall_plot(explainer.expected_value, shap_values[i], X_test.iloc[i], feature_names=feature_names, show=False)
        plt.title(f'SHAP Waterfall Plot - Sample {i+1}')
        plt.tight_layout()
        plt.show()
    
    return shap_values

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§ã®SHAPåˆ†æ
feature_names = X_test.columns.tolist()
shap_values = analyze_model_with_shap(optimized_xgb, X_test, feature_names)
```

### 7.2 äºˆæ¸¬çµæœã®è©³ç´°åˆ†æ

```python
# ã‚»ãƒ«13: äºˆæ¸¬çµæœã®åˆ†æ
def analyze_prediction_results(models, X_test, y_test):
    """äºˆæ¸¬çµæœã®è©³ç´°åˆ†æ"""
    
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
    best_models = {
        'Optimized XGBoost': optimized_xgb,
        'Optimized LightGBM': optimized_lgb
    }
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    for idx, (name, model) in enumerate(best_models.items()):
        y_pred = model.predict(X_test)
        
        # 1. äºˆæ¸¬ vs å®Ÿå€¤
        axes[idx, 0].scatter(y_test, y_pred, alpha=0.5)
        axes[idx, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        axes[idx, 0].set_xlabel('Actual')
        axes[idx, 0].set_ylabel('Predicted')
        axes[idx, 0].set_title(f'{name} - Actual vs Predicted')
        
        # RÂ² ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
        r2 = r2_score(y_test, y_pred)
        axes[idx, 0].text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=axes[idx, 0].transAxes, 
                         bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
        
        # 2. æ®‹å·®åˆ†æ
        residuals = y_test - y_pred
        axes[idx, 1].scatter(y_pred, residuals, alpha=0.5)
        axes[idx, 1].axhline(y=0, color='r', linestyle='--')
        axes[idx, 1].set_xlabel('Predicted')
        axes[idx, 1].set_ylabel('Residuals')
        axes[idx, 1].set_title(f'{name} - Residuals')
        
        # 3. ç€é †åˆ¥ã®äºˆæ¸¬ç²¾åº¦
        finish_positions = sorted(y_test.unique())[:8]  # ä¸Šä½8ç€ã¾ã§
        accuracy_by_finish = []
        
        for pos in finish_positions:
            mask = y_test == pos
            if mask.sum() > 10:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«æ•°ãŒã‚ã‚‹å ´åˆã®ã¿
                pred_for_pos = y_pred[mask]
                mae = mean_absolute_error([pos] * len(pred_for_pos), pred_for_pos)
                accuracy_by_finish.append(mae)
            else:
                accuracy_by_finish.append(np.nan)
        
        valid_positions = [pos for pos, acc in zip(finish_positions, accuracy_by_finish) if not np.isnan(acc)]
        valid_accuracies = [acc for acc in accuracy_by_finish if not np.isnan(acc)]
        
        if valid_accuracies:
            axes[idx, 2].bar(range(len(valid_positions)), valid_accuracies, color='skyblue')
            axes[idx, 2].set_xticks(range(len(valid_positions)))
            axes[idx, 2].set_xticklabels(valid_positions)
            axes[idx, 2].set_xlabel('ç€é †')
            axes[idx, 2].set_ylabel('MAE')
            axes[idx, 2].set_title(f'{name} - ç€é †åˆ¥MAE')
    
    plt.tight_layout()
    plt.show()
    
    # è©³ç´°ãªçµ±è¨ˆæƒ…å ±
    print("\n=== äºˆæ¸¬æ€§èƒ½è©³ç´° ===")
    for name, model in best_models.items():
        y_pred = model.predict(X_test)
        
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"\n{name}:")
        print(f"  MSE: {mse:.4f}")
        print(f"  MAE: {mae:.4f}")
        print(f"  RMSE: {np.sqrt(mse):.4f}")
        print(f"  RÂ²: {r2:.4f}")
        
        # ç€é †äºˆæ¸¬ã®ç²¾åº¦ï¼ˆÂ±1ç€ä»¥å†…ã®æ­£ç­”ç‡ï¼‰
        correct_within_1 = np.abs(y_test - y_pred) <= 1
        accuracy_within_1 = correct_within_1.mean()
        print(f"  Â±1ç€ä»¥å†…æ­£ç­”ç‡: {accuracy_within_1:.3f}")

# äºˆæ¸¬çµæœåˆ†æã®å®Ÿè¡Œ
analyze_prediction_results(ensemble_models, X_test, y_test)
```

## 8. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ 

### 8.1 äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

```python
# ã‚»ãƒ«14: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ 
import ipywidgets as widgets
from IPython.display import display, clear_output

class HorseRacePredictionSystem:
    """ç«¶é¦¬äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    def __init__(self, model, scaler, feature_columns):
        self.model = model
        self.scaler = scaler
        self.feature_columns = feature_columns
        
        # ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®ä½œæˆ
        self.create_widgets()
        self.create_interface()
    
    def create_widgets(self):
        """ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®ä½œæˆ"""
        
        # åŸºæœ¬æƒ…å ±å…¥åŠ›
        self.track_dropdown = widgets.Dropdown(
            options=['æ±äº¬', 'ä¸­å±±', 'äº¬éƒ½', 'é˜ªç¥', 'ä¸­äº¬', 'æ–°æ½Ÿ', 'ç¦å³¶', 'å°å€‰'],
            description='ç«¶é¦¬å ´:'
        )
        
        self.distance_slider = widgets.IntSlider(
            value=1600,
            min=1000,
            max=3600,
            step=200,
            description='è·é›¢ (m):'
        )
        
        self.track_condition = widgets.Dropdown(
            options=['è‰¯', 'ç¨é‡', 'é‡', 'ä¸è‰¯'],
            description='é¦¬å ´çŠ¶æ…‹:'
        )
        
        # é¦¬æƒ…å ±å…¥åŠ›
        self.horse_age = widgets.IntSlider(
            value=4,
            min=2,
            max=10,
            description='é¦¬é½¢:'
        )
        
        self.weight = widgets.IntSlider(
            value=55,
            min=50,
            max=65,
            description='æ–¤é‡ (kg):'
        )
        
        self.gate_number = widgets.IntSlider(
            value=8,
            min=1,
            max=18,
            description='æ ç•ª:'
        )
        
        self.odds = widgets.FloatSlider(
            value=5.0,
            min=1.0,
            max=100.0,
            step=0.1,
            description='ã‚ªãƒƒã‚º:'
        )
        
        self.popularity = widgets.IntSlider(
            value=5,
            min=1,
            max=18,
            description='äººæ°—:'
        )
        
        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        self.predict_button = widgets.Button(
            description='äºˆæƒ³å®Ÿè¡Œ',
            button_style='success',
            icon='play'
        )
        
        self.predict_button.on_click(self.make_prediction)
        
        # å‡ºåŠ›ã‚¨ãƒªã‚¢
        self.output = widgets.Output()
    
    def create_interface(self):
        """ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆä½œæˆ"""
        
        race_info = widgets.VBox([
            widgets.HTML('<h3>ãƒ¬ãƒ¼ã‚¹æƒ…å ±</h3>'),
            self.track_dropdown,
            self.distance_slider,
            self.track_condition
        ])
        
        horse_info = widgets.VBox([
            widgets.HTML('<h3>é¦¬æƒ…å ±</h3>'),
            self.horse_age,
            self.weight,
            self.gate_number,
            self.odds,
            self.popularity
        ])
        
        controls = widgets.HBox([race_info, horse_info])
        
        self.interface = widgets.VBox([
            widgets.HTML('<h2>ğŸ‡ ç«¶é¦¬AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ </h2>'),
            controls,
            self.predict_button,
            self.output
        ])
    
    def make_prediction(self, b):
        """äºˆæƒ³ã®å®Ÿè¡Œ"""
        
        with self.output:
            clear_output(wait=True)
            
            # å…¥åŠ›å€¤ã®å–å¾—
            features = self.collect_features()
            
            # äºˆæ¸¬å®Ÿè¡Œ
            try:
                prediction = self.model.predict([features])[0]
                confidence = self.calculate_confidence(features)
                
                # çµæœè¡¨ç¤º
                self.display_prediction(prediction, confidence, features)
                
            except Exception as e:
                print(f"äºˆæƒ³ã‚¨ãƒ©ãƒ¼: {e}")
    
    def collect_features(self):
        """ç‰¹å¾´é‡ã®åé›†ã¨å‰å‡¦ç†"""
        
        # åŸºæœ¬ç‰¹å¾´é‡
        features = {
            'horse_age': self.horse_age.value,
            'weight': self.weight.value,
            'gate_number': self.gate_number.value,
            'odds': self.odds.value,
            'popularity': self.popularity.value,
            'distance': self.distance_slider.value,
        }
        
        # æ´¾ç”Ÿç‰¹å¾´é‡ã®è¨ˆç®—
        features['odds_log'] = np.log1p(features['odds'])
        features['weight_normalized'] = (features['weight'] - 55) / 5
        features['gate_position_encoded'] = 1 if features['gate_number'] <= 4 else 2 if features['gate_number'] >= 13 else 0
        
        # ãƒˆãƒ©ãƒƒã‚¯æ¡ä»¶ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        track_conditions = {'è‰¯': 1, 'ç¨é‡': 0.8, 'é‡': 0.6, 'ä¸è‰¯': 0.4}
        features['track_condition_encoded'] = track_conditions.get(self.track_condition.value, 1)
        
        # ç«¶é¦¬å ´ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        track_mapping = {'æ±äº¬': 1, 'ä¸­å±±': 2, 'äº¬éƒ½': 3, 'é˜ªç¥': 4, 'ä¸­äº¬': 5, 'æ–°æ½Ÿ': 6, 'ç¦å³¶': 7, 'å°å€‰': 8}
        features['track_encoded'] = track_mapping.get(self.track_dropdown.value, 1)
        
        # ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ç‰¹å¾´é‡é †åºã«åˆã‚ã›ã‚‹
        feature_vector = []
        for col in self.feature_columns:
            if col in features:
                feature_vector.append(features[col])
            else:
                feature_vector.append(0)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        return feature_vector
    
    def calculate_confidence(self, features):
        """äºˆæƒ³ã®ä¿¡é ¼åº¦è¨ˆç®—"""
        # ç°¡æ˜“çš„ãªä¿¡é ¼åº¦è¨ˆç®—
        odds_factor = min(1.0, 10.0 / max(features[3], 1))  # ã‚ªãƒƒã‚ºãŒä½ã„ã»ã©ä¿¡é ¼åº¦é«˜
        popularity_factor = min(1.0, 10.0 / max(features[4], 1))  # äººæ°—ãŒé«˜ã„ã»ã©ä¿¡é ¼åº¦é«˜
        
        confidence = (odds_factor + popularity_factor) / 2
        return min(confidence * 100, 95)  # æœ€å¤§95%
    
    def display_prediction(self, prediction, confidence, features):
        """äºˆæƒ³çµæœã®è¡¨ç¤º"""
        
        print("ğŸ‡ äºˆæƒ³çµæœ")
        print("=" * 40)
        print(f"äºˆæƒ³ç€é †: {prediction:.1f}ç€")
        print(f"ä¿¡é ¼åº¦: {confidence:.1f}%")
        
        # ç€é †ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®åˆ¤å®š
        if prediction <= 1.5:
            category = "ğŸ¥‡ å‹åˆ©å€™è£œ"
            color = "gold"
        elif prediction <= 3.0:
            category = "ğŸ¥ˆ ä¸Šä½å…¥ç·š"
            color = "silver"
        elif prediction <= 6.0:
            category = "ğŸ¥‰ ä¸­ä½äº‰ã„"
            color = "orange"
        else:
            category = "ğŸ“‰ ä¸‹ä½äºˆæƒ³"
            color = "gray"
        
        print(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼: {category}")
        
        # è©³ç´°åˆ†æ
        print("\nğŸ“Š è©³ç´°åˆ†æ")
        print("-" * 20)
        print(f"ã‚ªãƒƒã‚º: {features[3]:.1f}å€")
        print(f"äººæ°—: {features[4]:.0f}äººæ°—")
        print(f"é¦¬é½¢: {features[0]:.0f}æ­³")
        print(f"æ–¤é‡: {features[1]:.0f}kg")
        print(f"æ ç•ª: {features[2]:.0f}ç•ª")
        
        # å¯è¦–åŒ–
        self.plot_prediction_analysis(prediction, features)
    
    def plot_prediction_analysis(self, prediction, features):
        """äºˆæƒ³åˆ†æã®ãƒ—ãƒ­ãƒƒãƒˆ"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # ç€é †åˆ†å¸ƒ
        positions = np.arange(1, 11)
        probabilities = np.exp(-(positions - prediction)**2 / 2) 
        probabilities = probabilities / probabilities.sum()
        
        bars = ax1.bar(positions, probabilities, color='skyblue', alpha=0.7)
        ax1.axvline(x=prediction, color='red', linestyle='--', linewidth=2, label=f'äºˆæƒ³ç€é †: {prediction:.1f}')
        ax1.set_xlabel('ç€é †')
        ax1.set_ylabel('ç¢ºç‡å¯†åº¦')
        ax1.set_title('ç€é †ç¢ºç‡åˆ†å¸ƒ')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ç‰¹å¾´é‡ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆä¸»è¦é …ç›®ã®ã¿ï¼‰
        categories = ['ã‚ªãƒƒã‚º\n(é€†æ•°)', 'äººæ°—\n(é€†æ•°)', 'é¦¬é½¢', 'æ–¤é‡', 'æ é †']
        
        # æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        values = [
            min(1.0, 10.0 / max(features[3], 1)),  # ã‚ªãƒƒã‚ºï¼ˆé€†æ•°ã§æ­£è¦åŒ–ï¼‰
            min(1.0, 10.0 / max(features[4], 1)),  # äººæ°—ï¼ˆé€†æ•°ã§æ­£è¦åŒ–ï¼‰
            features[0] / 10.0,                     # é¦¬é½¢
            (features[1] - 50) / 15.0,              # æ–¤é‡
            features[2] / 18.0                      # æ ç•ª
        ]
        
        # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)
        values += values[:1]  # å††ã‚’é–‰ã˜ã‚‹
        angles = np.concatenate((angles, [angles[0]]))
        
        ax2 = plt.subplot(122, polar=True)
        ax2.plot(angles, values, 'o-', linewidth=2, label='ç¾åœ¨ã®é¦¬', color='blue')
        ax2.fill(angles, values, alpha=0.25, color='blue')
        ax2.set_xticks(angles[:-1])
        ax2.set_xticklabels(categories)
        ax2.set_ylim(0, 1)
        ax2.set_title('ç‰¹å¾´é‡ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.show()
    
    def display(self):
        """ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®è¡¨ç¤º"""
        display(self.interface)

# äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆã¨è¡¨ç¤º
feature_columns = X_train.columns.tolist()
prediction_system = HorseRacePredictionSystem(optimized_xgb, scaler, feature_columns)
prediction_system.display()
```

### 8.2 ãƒãƒƒãƒäºˆæƒ³æ©Ÿèƒ½

```python
# ã‚»ãƒ«15: ãƒãƒƒãƒäºˆæƒ³æ©Ÿèƒ½
def batch_prediction_system(model, race_data_file):
    """è¤‡æ•°é¦¬ã®ä¸€æ‹¬äºˆæƒ³"""
    
    print("=== ãƒãƒƒãƒäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  ===")
    
    # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
    try:
        race_df = pd.read_csv(race_data_file)
        print(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(race_df)}é ­ã®é¦¬ãƒ‡ãƒ¼ã‚¿")
    except FileNotFoundError:
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        print("ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­...")
        race_df = create_sample_race_data()
    
    # ç‰¹å¾´é‡ã®å‰å‡¦ç†
    processed_df = preprocess_batch_data(race_df)
    
    # ä¸€æ‹¬äºˆæƒ³ã®å®Ÿè¡Œ
    predictions = model.predict(processed_df[feature_columns])
    
    # çµæœã®æ•´ç†
    results_df = race_df.copy()
    results_df['predicted_finish'] = predictions
    results_df['win_probability'] = calculate_win_probability(predictions)
    results_df['recommendation'] = get_betting_recommendation(predictions, results_df['odds'])
    
    # çµæœã®ã‚½ãƒ¼ãƒˆï¼ˆäºˆæƒ³ç€é †é †ï¼‰
    results_df = results_df.sort_values('predicted_finish')
    
    # çµæœè¡¨ç¤º
    display_batch_results(results_df)
    
    return results_df

def create_sample_race_data():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
    
    np.random.seed(42)
    
    sample_data = {
        'horse_name': [f'é¦¬{i+1}' for i in range(12)],
        'horse_age': np.random.randint(3, 8, 12),
        'weight': np.random.randint(52, 58, 12),
        'gate_number': range(1, 13),
        'odds': np.random.exponential(5, 12) + 1,
        'popularity': range(1, 13),
        'jockey': [f'é¨æ‰‹{i+1}' for i in range(12)],
        'trainer': [f'èª¿æ•™å¸«{i+1}' for i in range(12)]
    }
    
    return pd.DataFrame(sample_data)

def preprocess_batch_data(df):
    """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
    
    processed_df = df.copy()
    
    # æ´¾ç”Ÿç‰¹å¾´é‡ã®ä½œæˆ
    processed_df['odds_log'] = np.log1p(processed_df['odds'])
    processed_df['weight_normalized'] = (processed_df['weight'] - 55) / 5
    processed_df['gate_position_encoded'] = processed_df['gate_number'].apply(
        lambda x: 1 if x <= 4 else 2 if x >= 13 else 0
    )
    
    # ä¸è¶³ã™ã‚‹ç‰¹å¾´é‡ã‚’ãƒ€ãƒŸãƒ¼ã§åŸ‹ã‚ã‚‹
    for col in feature_columns:
        if col not in processed_df.columns:
            processed_df[col] = 0
    
    return processed_df

def calculate_win_probability(predictions):
    """å‹åˆ©ç¢ºç‡ã®è¨ˆç®—"""
    
    # ç€é †äºˆæƒ³ã‹ã‚‰å‹åˆ©ç¢ºç‡ã¸ã®å¤‰æ›
    win_probs = np.exp(-predictions + 1) / 5  # ç°¡æ˜“çš„ãªå¤‰æ›
    win_probs = np.clip(win_probs, 0, 1)
    
    return win_probs

def get_betting_recommendation(predictions, odds):
    """è³­ã‘æ¨å¥¨ã®è¨ˆç®—"""
    
    recommendations = []
    
    for pred, odd in zip(predictions, odds):
        expected_value = (1 / pred) * odd  # ç°¡æ˜“æœŸå¾…å€¤
        
        if pred <= 1.5 and expected_value > 1.2:
            recommendations.append("ğŸŸ¢ å¼·æ¨å¥¨")
        elif pred <= 3 and expected_value > 1.1:
            recommendations.append("ğŸŸ¡ æ¨å¥¨")
        elif pred <= 6:
            recommendations.append("âšª æ¤œè¨")
        else:
            recommendations.append("ğŸ”´ éæ¨å¥¨")
    
    return recommendations

def display_batch_results(results_df):
    """ãƒãƒƒãƒäºˆæƒ³çµæœã®è¡¨ç¤º"""
    
    print("\nğŸ‡ ãƒ¬ãƒ¼ã‚¹äºˆæƒ³çµæœ")
    print("=" * 80)
    
    # çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡¨ç¤º
    display_df = results_df[[
        'horse_name', 'predicted_finish', 'win_probability', 
        'odds', 'popularity', 'recommendation'
    ]].round(2)
    
    display_df.columns = ['é¦¬å', 'äºˆæƒ³ç€é †', 'å‹ç‡', 'ã‚ªãƒƒã‚º', 'äººæ°—', 'æ¨å¥¨']
    display(display_df)
    
    # å¯è¦–åŒ–
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. äºˆæƒ³ç€é † vs ã‚ªãƒƒã‚º
    scatter = ax1.scatter(
        results_df['predicted_finish'], 
        results_df['odds'], 
        c=results_df['win_probability'], 
        cmap='RdYlGn',
        s=100,
        alpha=0.7
    )
    ax1.set_xlabel('äºˆæƒ³ç€é †')
    ax1.set_ylabel('ã‚ªãƒƒã‚º')
    ax1.set_title('äºˆæƒ³ç€é † vs ã‚ªãƒƒã‚ºï¼ˆè‰²=å‹åˆ©ç¢ºç‡ï¼‰')
    plt.colorbar(scatter, ax=ax1, label='å‹åˆ©ç¢ºç‡')
    
    # é¦¬åã‚’ãƒ©ãƒ™ãƒ«è¡¨ç¤º
    for i, txt in enumerate(results_df['horse_name']):
        ax1.annotate(txt, (results_df['predicted_finish'].iloc[i], results_df['odds'].iloc[i]))
    
    # 2. å‹åˆ©ç¢ºç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    top_horses = results_df.nlargest(8, 'win_probability')
    bars = ax2.barh(range(len(top_horses)), top_horses['win_probability'], color='skyblue')
    ax2.set_yticks(range(len(top_horses)))
    ax2.set_yticklabels(top_horses['horse_name'])
    ax2.set_xlabel('å‹åˆ©ç¢ºç‡')
    ax2.set_title('å‹åˆ©ç¢ºç‡ Top 8')
    
    # ç¢ºç‡å€¤ã‚’ãƒãƒ¼ã«è¡¨ç¤º
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                f'{width:.3f}', ha='left', va='center')
    
    # 3. æ¨å¥¨åº¦åˆ†å¸ƒ
    recommendation_counts = results_df['recommendation'].value_counts()
    colors = ['red', 'orange', 'yellow', 'green'][:len(recommendation_counts)]
    ax3.pie(recommendation_counts.values, labels=recommendation_counts.index, 
            autopct='%1.0f%%', colors=colors)
    ax3.set_title('è³­ã‘æ¨å¥¨åº¦åˆ†å¸ƒ')
    
    # 4. äººæ°— vs äºˆæƒ³ç€é †
    ax4.scatter(results_df['popularity'], results_df['predicted_finish'], s=100, alpha=0.7)
    ax4.plot([1, results_df['popularity'].max()], [1, results_df['popularity'].max()], 
             'r--', alpha=0.5, label='äººæ°—=äºˆæƒ³ç€é †')
    ax4.set_xlabel('äººæ°—')
    ax4.set_ylabel('äºˆæƒ³ç€é †')
    ax4.set_title('äººæ°— vs äºˆæƒ³ç€é †')
    ax4.legend()
    
    # é¦¬åã‚’ãƒ©ãƒ™ãƒ«è¡¨ç¤º
    for i, txt in enumerate(results_df['horse_name']):
        ax4.annotate(txt, (results_df['popularity'].iloc[i], results_df['predicted_finish'].iloc[i]))
    
    plt.tight_layout()
    plt.show()
    
    # æŠ•è³‡æˆ¦ç•¥ã®ææ¡ˆ
    print("\nğŸ’° æŠ•è³‡æˆ¦ç•¥ææ¡ˆ")
    print("-" * 40)
    
    strong_recommendations = results_df[results_df['recommendation'] == "ğŸŸ¢ å¼·æ¨å¥¨"]
    if len(strong_recommendations) > 0:
        print("å¼·æ¨å¥¨é¦¬:")
        for _, horse in strong_recommendations.iterrows():
            roi = (horse['odds'] * horse['win_probability'] - 1) * 100
            print(f"  {horse['horse_name']}: äºˆæƒ³{horse['predicted_finish']:.1f}ç€, ROIæœŸå¾…å€¤ {roi:.1f}%")
    
    recommendations = results_df[results_df['recommendation'] == "ğŸŸ¡ æ¨å¥¨"]
    if len(recommendations) > 0:
        print("æ¨å¥¨é¦¬:")
        for _, horse in recommendations.iterrows():
            roi = (horse['odds'] * horse['win_probability'] - 1) * 100
            print(f"  {horse['horse_name']}: äºˆæƒ³{horse['predicted_finish']:.1f}ç€, ROIæœŸå¾…å€¤ {roi:.1f}%")

# ãƒãƒƒãƒäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œ
batch_results = batch_prediction_system(optimized_xgb, 'sample_race_data.csv')
```

## 9. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿

### 9.1 ãƒ¢ãƒ‡ãƒ«ã®æ°¸ç¶šåŒ–

```python
# ã‚»ãƒ«16: ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
import joblib
import pickle
from datetime import datetime
import json

def save_trained_model(model, scaler, feature_columns, model_name="horse_ai_model"):
    """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_dir = f"../models/{model_name}_{timestamp}"
    
    import os
    os.makedirs(model_dir, exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã®ä¿å­˜
    model_path = f"{model_dir}/model.joblib"
    joblib.dump(model, model_path)
    
    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¿å­˜
    scaler_path = f"{model_dir}/scaler.joblib"
    joblib.dump(scaler, scaler_path)
    
    # ç‰¹å¾´é‡æƒ…å ±ã®ä¿å­˜
    feature_info = {
        'feature_columns': feature_columns,
        'n_features': len(feature_columns),
        'model_type': type(model).__name__,
        'created_at': datetime.now().isoformat(),
        'training_data_shape': f"{X_train.shape[0]}x{X_train.shape[1]}"
    }
    
    with open(f"{model_dir}/feature_info.json", 'w', encoding='utf-8') as f:
        json.dump(feature_info, f, ensure_ascii=False, indent=2)
    
    # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æƒ…å ±ã®ä¿å­˜
    y_pred_test = model.predict(X_test)
    performance = {
        'test_mse': float(mean_squared_error(y_test, y_pred_test)),
        'test_mae': float(mean_absolute_error(y_test, y_pred_test)),
        'test_r2': float(r2_score(y_test, y_pred_test)),
        'feature_importance': dict(zip(feature_columns, 
                                     getattr(model, 'feature_importances_', [0]*len(feature_columns))))
    }
    
    with open(f"{model_dir}/performance.json", 'w') as f:
        json.dump(performance, f, indent=2)
    
    # READMEãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
    readme_content = f"""# ç«¶é¦¬AIäºˆæƒ³ãƒ¢ãƒ‡ãƒ«

## ãƒ¢ãƒ‡ãƒ«æƒ…å ±
- ä½œæˆæ—¥æ™‚: {datetime.now()}
- ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {type(model).__name__}
- ç‰¹å¾´é‡æ•°: {len(feature_columns)}
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X_train.shape[0]} samples

## æ€§èƒ½æŒ‡æ¨™
- Test MSE: {performance['test_mse']:.4f}
- Test MAE: {performance['test_mae']:.4f}
- Test RÂ²: {performance['test_r2']:.4f}

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
- `model.joblib`: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
- `scaler.joblib`: ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
- `feature_info.json`: ç‰¹å¾´é‡æƒ…å ±
- `performance.json`: æ€§èƒ½æŒ‡æ¨™
- `README.md`: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«

## ä½¿ç”¨æ–¹æ³•
```python
import joblib

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
model = joblib.load('model.joblib')
scaler = joblib.load('scaler.joblib')

# äºˆæ¸¬å®Ÿè¡Œ
predictions = model.predict(features)
```
"""
    
    with open(f"{model_dir}/README.md", 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_dir}")
    print(f"  - Model: {model_path}")
    print(f"  - Scaler: {scaler_path}")
    print(f"  - Feature Info: {feature_info}")
    print(f"  - Performance: {performance}")
    
    return model_dir

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
saved_model_dir = save_trained_model(
    optimized_xgb, 
    scaler, 
    feature_columns, 
    "optimized_xgboost"
)
```

### 9.2 ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨ä½¿ç”¨

```python
# ã‚»ãƒ«17: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
def load_trained_model(model_dir):
    """ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
    
    # ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã®èª­ã¿è¾¼ã¿
    model = joblib.load(f"{model_dir}/model.joblib")
    
    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®èª­ã¿è¾¼ã¿
    scaler = joblib.load(f"{model_dir}/scaler.joblib")
    
    # ç‰¹å¾´é‡æƒ…å ±ã®èª­ã¿è¾¼ã¿
    with open(f"{model_dir}/feature_info.json", 'r', encoding='utf-8') as f:
        feature_info = json.load(f)
    
    # æ€§èƒ½æƒ…å ±ã®èª­ã¿è¾¼ã¿
    with open(f"{model_dir}/performance.json", 'r') as f:
        performance = json.load(f)
    
    print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_dir}")
    print(f"  - ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {feature_info['model_type']}")
    print(f"  - ç‰¹å¾´é‡æ•°: {feature_info['n_features']}")
    print(f"  - ä½œæˆæ—¥æ™‚: {feature_info['created_at']}")
    print(f"  - Test RÂ²: {performance['test_r2']:.4f}")
    
    return model, scaler, feature_info, performance

# ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
loaded_model, loaded_scaler, loaded_feature_info, loaded_performance = load_trained_model(saved_model_dir)

# èª­ã¿è¾¼ã‚“ã ãƒ¢ãƒ‡ãƒ«ã§ã®äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
test_predictions = loaded_model.predict(X_test)
print(f"\nèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆæˆåŠŸ - äºˆæ¸¬æ•°: {len(test_predictions)}")
print(f"äºˆæ¸¬ä¾‹: {test_predictions[:5]}")
```

## 10. ç¶™ç¶šçš„æ”¹å–„ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### 10.1 ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
# ã‚»ãƒ«18: æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
class ModelPerformanceMonitor:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
    
    def __init__(self, model, reference_data):
        self.model = model
        self.reference_X = reference_data[0]
        self.reference_y = reference_data[1]
        self.performance_history = []
        
    def evaluate_performance(self, new_X, new_y, date=None):
        """æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½è©•ä¾¡"""
        
        if date is None:
            date = datetime.now()
        
        predictions = self.model.predict(new_X)
        
        performance = {
            'date': date,
            'mse': mean_squared_error(new_y, predictions),
            'mae': mean_absolute_error(new_y, predictions),
            'r2': r2_score(new_y, predictions),
            'sample_size': len(new_y)
        }
        
        self.performance_history.append(performance)
        
        return performance
    
    def detect_drift(self, new_performance, threshold=0.1):
        """æ€§èƒ½åŠ£åŒ–ã®æ¤œå‡º"""
        
        if len(self.performance_history) < 2:
            return False, "å±¥æ­´ä¸è¶³"
        
        baseline_r2 = self.performance_history[0]['r2']
        current_r2 = new_performance['r2']
        
        drift_magnitude = baseline_r2 - current_r2
        
        if drift_magnitude > threshold:
            return True, f"æ€§èƒ½åŠ£åŒ–æ¤œå‡º: RÂ²ãŒ{drift_magnitude:.3f}ä½ä¸‹"
        
        return False, "æ€§èƒ½å®‰å®š"
    
    def plot_performance_trend(self):
        """æ€§èƒ½æ¨ç§»ã®å¯è¦–åŒ–"""
        
        if not self.performance_history:
            print("ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        df = pd.DataFrame(self.performance_history)
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # RÂ²ã®æ¨ç§»
        ax1.plot(df.index, df['r2'], marker='o', linewidth=2, markersize=6)
        ax1.set_title('RÂ² Score æ¨ç§»')
        ax1.set_ylabel('RÂ² Score')
        ax1.grid(True, alpha=0.3)
        
        # MSEã®æ¨ç§»
        ax2.plot(df.index, df['mse'], marker='s', linewidth=2, markersize=6, color='red')
        ax2.set_title('MSE æ¨ç§»')
        ax2.set_ylabel('MSE')
        ax2.grid(True, alpha=0.3)
        
        # MAEã®æ¨ç§»
        ax3.plot(df.index, df['mae'], marker='^', linewidth=2, markersize=6, color='green')
        ax3.set_title('MAE æ¨ç§»')
        ax3.set_ylabel('MAE')
        ax3.set_xlabel('è©•ä¾¡å›æ•°')
        ax3.grid(True, alpha=0.3)
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®æ¨ç§»
        ax4.bar(df.index, df['sample_size'], alpha=0.7, color='orange')
        ax4.set_title('ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º')
        ax4.set_ylabel('ã‚µãƒ³ãƒ—ãƒ«æ•°')
        ax4.set_xlabel('è©•ä¾¡å›æ•°')
        
        plt.tight_layout()
        plt.show()
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        print("\n=== æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° ã‚µãƒãƒªãƒ¼ ===")
        print(f"è©•ä¾¡å›æ•°: {len(df)}")
        print(f"RÂ² - æœ€å¤§: {df['r2'].max():.4f}, æœ€å°: {df['r2'].min():.4f}, å¹³å‡: {df['r2'].mean():.4f}")
        print(f"MSE - æœ€å¤§: {df['mse'].max():.4f}, æœ€å°: {df['mse'].min():.4f}, å¹³å‡: {df['mse'].mean():.4f}")
        print(f"MAE - æœ€å¤§: {df['mae'].max():.4f}, æœ€å°: {df['mae'].min():.4f}, å¹³å‡: {df['mae'].mean():.4f}")

# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
monitor = ModelPerformanceMonitor(optimized_xgb, (X_test, y_test))

# åˆå›è©•ä¾¡
initial_performance = monitor.evaluate_performance(X_test, y_test)
print("åˆå›æ€§èƒ½è©•ä¾¡:")
print(f"  RÂ²: {initial_performance['r2']:.4f}")
print(f"  MSE: {initial_performance['mse']:.4f}")
print(f"  MAE: {initial_performance['mae']:.4f}")

# æ€§èƒ½æ¨ç§»ã®å¯è¦–åŒ–
monitor.plot_performance_trend()
```

### 10.2 è‡ªå‹•å†å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 

```python
# ã‚»ãƒ«19: è‡ªå‹•å†å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
class AutoRetrainingSystem:
    """è‡ªå‹•å†å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_class, base_params, retrain_threshold=0.1):
        self.model_class = model_class
        self.base_params = base_params
        self.retrain_threshold = retrain_threshold
        self.training_history = []
        
    def should_retrain(self, current_performance, baseline_performance):
        """å†å­¦ç¿’ãŒå¿…è¦ã‹ã©ã†ã‹ã®åˆ¤å®š"""
        
        performance_drop = baseline_performance['r2'] - current_performance['r2']
        
        return performance_drop > self.retrain_threshold
    
    def retrain_model(self, X_train, y_train, X_val, y_val):
        """ãƒ¢ãƒ‡ãƒ«ã®å†å­¦ç¿’"""
        
        print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’é–‹å§‹...")
        
        # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        new_model = self.model_class(**self.base_params)
        new_model.fit(X_train, y_train)
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
        val_predictions = new_model.predict(X_val)
        new_performance = {
            'r2': r2_score(y_val, val_predictions),
            'mse': mean_squared_error(y_val, val_predictions),
            'mae': mean_absolute_error(y_val, val_predictions),
            'retrain_date': datetime.now()
        }
        
        self.training_history.append(new_performance)
        
        print(f"âœ… å†å­¦ç¿’å®Œäº†")
        print(f"   æ–°ã—ã„RÂ²: {new_performance['r2']:.4f}")
        print(f"   æ–°ã—ã„MSE: {new_performance['mse']:.4f}")
        
        return new_model, new_performance
    
    def incremental_learning(self, current_model, new_X, new_y):
        """å¢—åˆ†å­¦ç¿’ï¼ˆå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å ´åˆï¼‰"""
        
        try:
            # ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã¯éƒ¨åˆ†å­¦ç¿’ã‚’ã‚µãƒãƒ¼ãƒˆ
            if hasattr(current_model, 'partial_fit'):
                current_model.partial_fit(new_X, new_y)
                print("âœ… å¢—åˆ†å­¦ç¿’å®Œäº†")
                return current_model
            else:
                print("âš ï¸ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯å¢—åˆ†å­¦ç¿’ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“")
                return current_model
        except Exception as e:
            print(f"âŒ å¢—åˆ†å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return current_model
    
    def auto_model_selection(self, X_train, y_train, X_val, y_val):
        """è‡ªå‹•ãƒ¢ãƒ‡ãƒ«é¸æŠ"""
        
        models = {
            'XGBoost': xgb.XGBRegressor(**self.base_params),
            'LightGBM': lgb.LGBMRegressor(**self.base_params, verbosity=-1),
            'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
        }
        
        best_model = None
        best_score = -np.inf
        best_name = None
        
        print("ğŸ” æœ€é©ãƒ¢ãƒ‡ãƒ«æ¢ç´¢ä¸­...")
        
        for name, model in models.items():
            try:
                model.fit(X_train, y_train)
                val_pred = model.predict(X_val)
                score = r2_score(y_val, val_pred)
                
                print(f"  {name}: RÂ² = {score:.4f}")
                
                if score > best_score:
                    best_score = score
                    best_model = model
                    best_name = name
                    
            except Exception as e:
                print(f"  {name}: ã‚¨ãƒ©ãƒ¼ - {e}")
        
        print(f"âœ… æœ€é©ãƒ¢ãƒ‡ãƒ«: {best_name} (RÂ² = {best_score:.4f})")
        
        return best_model, best_name, best_score

# è‡ªå‹•å†å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
auto_retrain = AutoRetrainingSystem(
    model_class=xgb.XGBRegressor,
    base_params={
        'n_estimators': 100,
        'max_depth': 6,
        'learning_rate': 0.1,
        'random_state': 42
    }
)

# å†å­¦ç¿’ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
print("=== è‡ªå‹•å†å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢ ===")

# ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ï¼ˆæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
X_retrain, X_new_val, y_retrain, y_new_val = train_test_split(
    X_train, y_train, test_size=0.3, random_state=123
)

# ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡
current_perf = {
    'r2': r2_score(y_test, optimized_xgb.predict(X_test)),
    'mse': mean_squared_error(y_test, optimized_xgb.predict(X_test)),
    'mae': mean_absolute_error(y_test, optimized_xgb.predict(X_test))
}

print(f"ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½: RÂ² = {current_perf['r2']:.4f}")

# è‡ªå‹•ãƒ¢ãƒ‡ãƒ«é¸æŠã®å®Ÿè¡Œ
new_model, model_name, model_score = auto_retrain.auto_model_selection(
    X_retrain, y_retrain, X_new_val, y_new_val
)

print(f"\næ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {model_name}ã¸ã®æ›´æ–° (æ€§èƒ½å‘ä¸Š: {model_score - current_perf['r2']:.4f})")
```

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€Jupyter Notebookã‚’ä½¿ç”¨ã—ãŸç«¶é¦¬AIé–‹ç™ºã®åŒ…æ‹¬çš„ãªç’°å¢ƒæ§‹ç¯‰ã¨å®Ÿè£…æ‰‹æ³•ã‚’è§£èª¬ã—ã¾ã—ãŸã€‚ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

### é–‹ç™ºç’°å¢ƒã®å„ªä½æ€§
1. **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™º**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã¨ä»®èª¬æ¤œè¨¼
2. **å¯è¦–åŒ–çµ±åˆ**: åˆ†æçµæœã®ç›´æ„Ÿçš„ãªç†è§£ã‚’ä¿ƒé€²
3. **å®Ÿé¨“ç®¡ç†**: ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å½¢å¼ã§ã®çŸ¥è¦‹è“„ç©ã¨å…±æœ‰
4. **ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°**: è¿…é€Ÿãªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã¨æ¤œè¨¼ã‚µã‚¤ã‚¯ãƒ«

### å®Ÿè£…ã—ãŸä¸»è¦æ©Ÿèƒ½
1. **ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ãƒ»å‰å‡¦ç†**: åŠ¹ç‡çš„ãªEDAã¨å“è³ªç®¡ç†
2. **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: é«˜åº¦ãªæ´¾ç”Ÿç‰¹å¾´é‡ã®è‡ªå‹•ç”Ÿæˆ
3. **æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«**: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
4. **ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ**: SHAP ã«ã‚ˆã‚‹èª¬æ˜å¯èƒ½AI ã®å®Ÿè£…
5. **äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ **: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ»ãƒãƒƒãƒä¸¡å¯¾å¿œã®äºˆæƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
6. **ç¶™ç¶šçš„æ”¹å–„**: æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨è‡ªå‹•å†å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 

### æŠ€è¡“çš„ç‰¹å¾´
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¸ã®å¯¾å¿œ
- **èª¬æ˜å¯èƒ½æ€§**: äºˆæƒ³æ ¹æ‹ ã®æ˜ç¢ºåŒ–ã¨ä¿¡é ¼æ€§å‘ä¸Š
- **è‡ªå‹•åŒ–**: ç¶™ç¶šçš„ãªå­¦ç¿’ã¨ãƒ¢ãƒ‡ãƒ«æ›´æ–°
- **å®Ÿç”¨æ€§**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæƒ³ã¨ãƒãƒƒãƒå‡¦ç†ã®ä¸¡ç«‹

Jupyter Notebookç’°å¢ƒã¯ã€ç ”ç©¶é–‹ç™ºã‹ã‚‰æœ¬æ ¼é‹ç”¨ã¾ã§ä¸€è²«ã—ã¦ã‚µãƒãƒ¼ãƒˆã™ã‚‹å¼·åŠ›ãªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚æœ¬è¨˜äº‹ã§ç¤ºã—ãŸæ‰‹æ³•ã‚’åŸºã«ã€ã•ã‚‰ã«é«˜åº¦ãªåˆ†ææ‰‹æ³•ã‚„æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„ç«¶é¦¬AI ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ã„ãã“ã¨ãŒã§ãã‚‹ã§ã—ã‚‡ã†ã€‚

ç«¶é¦¬AIé–‹ç™ºã«ãŠã„ã¦ã¯ã€ç¶™ç¶šçš„ãªãƒ‡ãƒ¼ã‚¿åé›†ã€ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã€ãã—ã¦å®Ÿé‹ç”¨ã§ã®æ¤œè¨¼ãŒæˆåŠŸã®éµã¨ãªã‚Šã¾ã™ã€‚Jupyter Notebookã®æŸ”è»Ÿæ€§ã‚’æ´»ã‹ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å¾“ã£ã¦ã€æŒç¶šå¯èƒ½ã§åŠ¹æœçš„ãªäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã‚’ç›®æŒ‡ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚