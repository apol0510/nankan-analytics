---
title: "ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§äºˆæƒ³ç²¾åº¦ã‚’æ­£ã—ãè©•ä¾¡ã™ã‚‹æ–¹æ³•"
date: 2025-08-07T13:00:00+09:00
description: "ç«¶é¦¬äºˆæƒ³ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ­£ç¢ºã«è©•ä¾¡ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•ã‚’å¾¹åº•è§£èª¬ã€‚æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸè©•ä¾¡æ–¹æ³•ã‹ã‚‰å®Ÿç”¨çš„ãªæ¤œè¨¼ã¾ã§ã€å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã¨ã¨ã‚‚ã«ç´¹ä»‹ã—ã¾ã™ã€‚"
categories: ["æ©Ÿæ¢°å­¦ç¿’", "ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"]
tags: ["ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³", "ãƒ¢ãƒ‡ãƒ«è©•ä¾¡", "ç«¶é¦¬äºˆæƒ³", "æ™‚ç³»åˆ—", "éå­¦ç¿’", "æ±åŒ–æ€§èƒ½"]
slug: "cross-validation-horse-prediction"
---

## ã¯ã˜ã‚ã«

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ãŠã„ã¦ã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆäº¤å·®æ¤œè¨¼ï¼‰ã¯æ¥µã‚ã¦é‡è¦ãªæŠ€è¡“ã§ã™ã€‚ç‰¹ã«ç«¶é¦¬äºˆæƒ³ã®ã‚ˆã†ãªæ™‚ç³»åˆ—æ€§ã¨ãƒ¬ãƒ¼ã‚¹ç‰¹æœ‰ã®æ§‹é€ ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€é©åˆ‡ãªè©•ä¾¡æ‰‹æ³•ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒãƒ¢ãƒ‡ãƒ«ã®å®Ÿç”¨æ€§ã‚’æ±ºå®šã—ã¾ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã«ç‰¹åŒ–ã—ãŸã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•ã‚’è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚

## ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®åŸºç¤ç†è«–

### ãªãœã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã‹

å˜ç´”ãªè¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²ã§ã¯ä»¥ä¸‹ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ï¼š

- **ãƒ‡ãƒ¼ã‚¿ã®åã‚Š**: å¶ç„¶ã®åˆ†å‰²ã«ã‚ˆã‚‹æ€§èƒ½ã®åã‚Š
- **éå­¦ç¿’ã®è¦‹è½ã¨ã—**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¸ã®éé©åˆã®ç™ºè¦‹ãŒå›°é›£
- **æ±åŒ–æ€§èƒ½ã®éå¤§è©•ä¾¡**: é™å®šçš„ãªãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®æ¥½è¦³çš„è©•ä¾¡

### ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®èª²é¡Œ

ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®ç‰¹æ®Šæ€§ãŒã‚ã‚Šã¾ã™ï¼š

- **æ™‚ç³»åˆ—ä¾å­˜æ€§**: éå»ã®ãƒ‡ãƒ¼ã‚¿ã§æœªæ¥ã‚’äºˆæ¸¬
- **ãƒ¬ãƒ¼ã‚¹æ§‹é€ **: åŒä¸€ãƒ¬ãƒ¼ã‚¹å†…ã§ã®ç›¸äº’ä¾å­˜
- **å­£ç¯€æ€§**: ç«¶é¦¬å ´ãƒ»è·é›¢ãƒ»æ™‚æœŸã«ã‚ˆã‚‹å‘¨æœŸæ€§
- **ãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡**: å‹åˆ©é¦¬ã¯ç´„8-10%ã®ã¿

## ç’°å¢ƒæº–å‚™

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# sklearn
from sklearn.model_selection import (
    KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold,
    cross_val_score, cross_validate, validation_curve, learning_curve
)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix
)
from sklearn.preprocessing import StandardScaler, LabelEncoder

# çµ±è¨ˆ
from scipy import stats
from scipy.stats import ttest_rel

# å¯è¦–åŒ–è¨­å®š
plt.rcParams['figure.figsize'] = (12, 8)
sns.set_style("whitegrid")

print("ç’°å¢ƒæº–å‚™å®Œäº†")
```

## ãƒ‡ãƒ¼ã‚¿æº–å‚™

```python
# ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆæ™‚ç³»åˆ—æ€§ã‚’è€ƒæ…®ï¼‰
np.random.seed(42)
n_races = 2000
n_horses_per_race = 10
start_date = datetime(2020, 1, 1)

data = []
for race_id in range(n_races):
    # æ™‚ç³»åˆ—é †ã§ãƒ¬ãƒ¼ã‚¹æ—¥ã‚’ç”Ÿæˆ
    race_date = start_date + timedelta(days=race_id // 10)  # 1æ—¥10ãƒ¬ãƒ¼ã‚¹ç¨‹åº¦
    
    # å­£ç¯€æ€§ã‚’è€ƒæ…®ã—ãŸç‰¹å¾´
    month = race_date.month
    season_factor = np.sin(2 * np.pi * month / 12)  # å­£ç¯€æ€§
    
    track = np.random.choice(['æ±äº¬', 'ä¸­å±±', 'é˜ªç¥', 'äº¬éƒ½'])
    distance = np.random.choice([1200, 1400, 1600, 1800, 2000, 2400])
    
    for horse_pos in range(n_horses_per_race):
        popularity = horse_pos + 1
        
        # æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¿½åŠ ï¼ˆäºˆæƒ³ç²¾åº¦ã®å‘ä¸Šãªã©ï¼‰
        time_trend = race_id / n_races * 0.1
        
        # å­£ç¯€æ€§ã®å½±éŸ¿
        seasonal_effect = season_factor * 0.05
        
        data.append({
            'race_id': race_id,
            'race_date': race_date,
            'year': race_date.year,
            'month': race_date.month,
            'day_of_year': race_date.timetuple().tm_yday,
            'track': track,
            'distance': distance,
            'horse_id': f'H{race_id:04d}{horse_pos:02d}',
            'popularity': popularity,
            'odds': 1.5 + popularity * 2 + np.random.exponential(2) + seasonal_effect,
            'weight': 460 + np.random.normal(0, 20),
            'age': np.random.randint(3, 8),
            'sex': np.random.choice(['ç‰¡', 'ç‰', 'ã‚»'], p=[0.5, 0.4, 0.1]),
            'jockey_skill': np.random.uniform(0.8, 1.2) + time_trend,
            'trainer_skill': np.random.uniform(0.9, 1.1) + time_trend,
            'recent_form': np.random.uniform(0.7, 1.3),
            'track_condition': np.random.choice(['è‰¯', 'ç¨é‡', 'é‡', 'ä¸è‰¯'], p=[0.7, 0.2, 0.07, 0.03])
        })

# ç€é †æ±ºå®šï¼ˆäººæ°—ã¨ã‚¹ã‚­ãƒ«ã«åŸºã¥ãç¢ºç‡çš„ãƒ¢ãƒ‡ãƒ«ï¼‰
for race_data in [data[i:i+n_horses_per_race] for i in range(0, len(data), n_horses_per_race)]:
    # å„é¦¬ã®å®ŸåŠ›è¨ˆç®—
    for horse in race_data:
        base_ability = (
            - horse['popularity'] * 0.3  # äººæ°—é¦¬ã»ã©å¼·ã„
            + horse['jockey_skill'] * 0.4
            + horse['trainer_skill'] * 0.3
            + horse['recent_form'] * 0.2
            + np.random.normal(0, 0.5)  # ãƒ©ãƒ³ãƒ€ãƒ è¦ç´ 
        )
        horse['ability'] = base_ability
    
    # ç€é †æ±ºå®š
    race_data.sort(key=lambda x: x['ability'], reverse=True)
    for i, horse in enumerate(race_data):
        horse['finish_position'] = i + 1
        horse['is_winner'] = 1 if i == 0 else 0

df = pd.DataFrame(data)
df['race_date'] = pd.to_datetime(df['race_date'])

print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.shape}")
print(f"æœŸé–“: {df['race_date'].min()} - {df['race_date'].max()}")
print(f"å‹ç‡: {df['is_winner'].mean():.3f}")

# åŸºæœ¬çš„ãªå¯è¦–åŒ–
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# æ™‚ç³»åˆ—ã§ã®å‹ç‡å¤‰åŒ–
monthly_win_rate = df.groupby(df['race_date'].dt.to_period('M'))['is_winner'].mean()
monthly_win_rate.plot(ax=axes[0,0])
axes[0,0].set_title('æœˆåˆ¥å‹ç‡ã®å¤‰åŒ–')
axes[0,0].set_ylabel('å‹ç‡')

# äººæ°—åˆ¥å‹ç‡
popularity_win_rate = df.groupby('popularity')['is_winner'].mean()
popularity_win_rate.plot(kind='bar', ax=axes[0,1])
axes[0,1].set_title('äººæ°—åˆ¥å‹ç‡')
axes[0,1].set_ylabel('å‹ç‡')

# ãƒ¬ãƒ¼ã‚¹æ•°ã®æ™‚ç³»åˆ—å¤‰åŒ–
daily_races = df.groupby(df['race_date'].dt.date).size()
daily_races.plot(ax=axes[1,0])
axes[1,0].set_title('æ—¥åˆ¥ãƒ¬ãƒ¼ã‚¹æ•°')
axes[1,0].set_ylabel('ãƒ¬ãƒ¼ã‚¹æ•°')

# ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ
df[['odds', 'weight', 'age']].hist(ax=axes[1,1], alpha=0.7)
axes[1,1].set_title('ä¸»è¦å¤‰æ•°ã®åˆ†å¸ƒ')

plt.tight_layout()
plt.show()
```

## åŸºæœ¬çš„ãªã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•

### 1. K-Foldäº¤å·®æ¤œè¨¼

```python
def basic_cross_validation_comparison(df):
    """åŸºæœ¬çš„ãªã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•ã®æ¯”è¼ƒ"""
    
    # ç‰¹å¾´é‡æº–å‚™
    feature_columns = ['popularity', 'odds', 'weight', 'age', 'jockey_skill', 
                      'trainer_skill', 'recent_form', 'distance']
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    le_sex = LabelEncoder()
    le_track = LabelEncoder()
    le_condition = LabelEncoder()
    
    df_encoded = df.copy()
    df_encoded['sex_encoded'] = le_sex.fit_transform(df['sex'])
    df_encoded['track_encoded'] = le_track.fit_transform(df['track'])
    df_encoded['condition_encoded'] = le_condition.fit_transform(df['track_condition'])
    
    feature_columns.extend(['sex_encoded', 'track_encoded', 'condition_encoded'])
    
    X = df_encoded[feature_columns]
    y = df_encoded['is_winner']
    
    # å„ç¨®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•
    cv_methods = {
        'K-Fold (5)': KFold(n_splits=5, shuffle=True, random_state=42),
        'Stratified K-Fold (5)': StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        'K-Fold (10)': KFold(n_splits=10, shuffle=True, random_state=42),
        'Stratified K-Fold (10)': StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    }
    
    # ãƒ¢ãƒ‡ãƒ«
    models = {
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(random_state=42)
    }
    
    results = []
    
    print("=== åŸºæœ¬çš„ãªã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ¯”è¼ƒ ===")
    print(f"{'CV Method':<25} {'Model':<20} {'Accuracy':<10} {'AUC':<10} {'Std':<10}")
    print("-" * 80)
    
    for cv_name, cv_method in cv_methods.items():
        for model_name, model in models.items():
            # è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã®è©•ä¾¡
            scores = cross_validate(
                model, X, y, cv=cv_method,
                scoring=['accuracy', 'roc_auc'],
                return_train_score=False
            )
            
            acc_mean = scores['test_accuracy'].mean()
            acc_std = scores['test_accuracy'].std()
            auc_mean = scores['test_roc_auc'].mean()
            auc_std = scores['test_roc_auc'].std()
            
            results.append({
                'cv_method': cv_name,
                'model': model_name,
                'accuracy_mean': acc_mean,
                'accuracy_std': acc_std,
                'auc_mean': auc_mean,
                'auc_std': auc_std
            })
            
            print(f"{cv_name:<25} {model_name:<20} {acc_mean:<10.3f} {auc_mean:<10.3f} {acc_std:<10.3f}")
    
    return pd.DataFrame(results), X, y

cv_results, X, y = basic_cross_validation_comparison(df)
```

### 2. æ™‚ç³»åˆ—ã‚’è€ƒæ…®ã—ãŸäº¤å·®æ¤œè¨¼

```python
def time_aware_cross_validation(df, X, y):
    """æ™‚ç³»åˆ—ã‚’è€ƒæ…®ã—ãŸã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("\n=== æ™‚ç³»åˆ—ã‚’è€ƒæ…®ã—ãŸã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ ===")
    
    # æ™‚ç³»åˆ—åˆ†å‰²ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    df_with_features = pd.concat([df[['race_id', 'race_date']], X, y], axis=1)
    df_sorted = df_with_features.sort_values('race_date')
    
    X_sorted = df_sorted.drop(['race_id', 'race_date', 'is_winner'], axis=1)
    y_sorted = df_sorted['is_winner']
    
    # 1. TimeSeriesSplit
    tscv = TimeSeriesSplit(n_splits=5)
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # æ™‚ç³»åˆ—åˆ†å‰²ã§ã®è©•ä¾¡
    ts_scores = cross_validate(
        model, X_sorted, y_sorted, cv=tscv,
        scoring=['accuracy', 'roc_auc'],
        return_train_score=True
    )
    
    print("TimeSeriesSplitçµæœ:")
    print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {ts_scores['test_accuracy'].mean():.3f} Â± {ts_scores['test_accuracy'].std():.3f}")
    print(f"ãƒ†ã‚¹ãƒˆAUC: {ts_scores['test_roc_auc'].mean():.3f} Â± {ts_scores['test_roc_auc'].std():.3f}")
    print(f"è¨“ç·´ç²¾åº¦: {ts_scores['train_accuracy'].mean():.3f} Â± {ts_scores['train_accuracy'].std():.3f}")
    
    # 2. æ‰‹å‹•æ™‚ç³»åˆ—åˆ†å‰²
    def manual_time_split(df, n_splits=5):
        """æ‰‹å‹•ã§ã®æ™‚ç³»åˆ—åˆ†å‰²"""
        df_sorted = df.sort_values('race_date')
        total_samples = len(df_sorted)
        
        splits = []
        for i in range(n_splits):
            # è¨“ç·´æœŸé–“ã®çµ‚äº†ç‚¹
            train_end = int(total_samples * (i + 2) / (n_splits + 1))
            # ãƒ†ã‚¹ãƒˆæœŸé–“ã®é–‹å§‹ãƒ»çµ‚äº†ç‚¹
            test_start = train_end
            test_end = int(total_samples * (i + 3) / (n_splits + 1))
            
            train_indices = df_sorted.index[:train_end].tolist()
            test_indices = df_sorted.index[test_start:test_end].tolist()
            
            splits.append((train_indices, test_indices))
            
            train_period = df_sorted.iloc[:train_end]['race_date'].agg(['min', 'max'])
            test_period = df_sorted.iloc[test_start:test_end]['race_date'].agg(['min', 'max'])
            
            print(f"Fold {i+1}:")
            print(f"  è¨“ç·´æœŸé–“: {train_period['min'].date()} - {train_period['max'].date()}")
            print(f"  ãƒ†ã‚¹ãƒˆæœŸé–“: {test_period['min'].date()} - {test_period['max'].date()}")
        
        return splits
    
    # æ‰‹å‹•åˆ†å‰²ã§ã®è©•ä¾¡
    manual_splits = manual_time_split(df_with_features, n_splits=5)
    
    manual_scores = []
    for train_idx, test_idx in manual_splits:
        X_train_fold = X.loc[train_idx]
        X_test_fold = X.loc[test_idx]
        y_train_fold = y.loc[train_idx]
        y_test_fold = y.loc[test_idx]
        
        model.fit(X_train_fold, y_train_fold)
        
        train_pred = model.predict(X_train_fold)
        test_pred = model.predict(X_test_fold)
        test_proba = model.predict_proba(X_test_fold)[:, 1]
        
        manual_scores.append({
            'train_accuracy': accuracy_score(y_train_fold, train_pred),
            'test_accuracy': accuracy_score(y_test_fold, test_pred),
            'test_auc': roc_auc_score(y_test_fold, test_proba)
        })
    
    manual_df = pd.DataFrame(manual_scores)
    print(f"\næ‰‹å‹•æ™‚ç³»åˆ—åˆ†å‰²çµæœ:")
    print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {manual_df['test_accuracy'].mean():.3f} Â± {manual_df['test_accuracy'].std():.3f}")
    print(f"ãƒ†ã‚¹ãƒˆAUC: {manual_df['test_auc'].mean():.3f} Â± {manual_df['test_auc'].std():.3f}")
    
    return ts_scores, manual_scores

ts_scores, manual_scores = time_aware_cross_validation(df, X, y)
```

### 3. ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è€ƒæ…®ã—ãŸäº¤å·®æ¤œè¨¼

```python
def group_cross_validation(df, X, y):
    """ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è€ƒæ…®ã—ãŸã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("\n=== ã‚°ãƒ«ãƒ¼ãƒ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ ===")
    
    # ãƒ¬ãƒ¼ã‚¹IDã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã¨ã—ã¦ä½¿ç”¨
    groups = df['race_id']
    
    # GroupKFold
    gkf = GroupKFold(n_splits=5)
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # ã‚°ãƒ«ãƒ¼ãƒ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    group_scores = cross_validate(
        model, X, y, groups=groups, cv=gkf,
        scoring=['accuracy', 'roc_auc'],
        return_train_score=True
    )
    
    print("GroupKFoldçµæœ:")
    print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {group_scores['test_accuracy'].mean():.3f} Â± {group_scores['test_accuracy'].std():.3f}")
    print(f"ãƒ†ã‚¹ãƒˆAUC: {group_scores['test_roc_auc'].mean():.3f} Â± {group_scores['test_roc_auc'].std():.3f}")
    
    # æ‰‹å‹•ã§ã®ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†å‰²
    def race_level_split(df, n_splits=5):
        """ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã§ã®åˆ†å‰²"""
        unique_races = df['race_id'].unique()
        np.random.shuffle(unique_races)
        
        race_splits = np.array_split(unique_races, n_splits)
        
        splits = []
        for i in range(n_splits):
            test_races = race_splits[i]
            train_races = np.concatenate([race_splits[j] for j in range(n_splits) if j != i])
            
            train_mask = df['race_id'].isin(train_races)
            test_mask = df['race_id'].isin(test_races)
            
            train_indices = df[train_mask].index.tolist()
            test_indices = df[test_mask].index.tolist()
            
            splits.append((train_indices, test_indices))
            
            print(f"Fold {i+1}: è¨“ç·´ãƒ¬ãƒ¼ã‚¹{len(train_races)}å€‹, ãƒ†ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¹{len(test_races)}å€‹")
        
        return splits
    
    race_splits = race_level_split(df, n_splits=5)
    
    # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«è©•ä¾¡
    race_scores = []
    for train_idx, test_idx in race_splits:
        X_train_fold = X.iloc[train_idx]
        X_test_fold = X.iloc[test_idx]
        y_train_fold = y.iloc[train_idx]
        y_test_fold = y.iloc[test_idx]
        
        model.fit(X_train_fold, y_train_fold)
        
        test_pred = model.predict(X_test_fold)
        test_proba = model.predict_proba(X_test_fold)[:, 1]
        
        race_scores.append({
            'accuracy': accuracy_score(y_test_fold, test_pred),
            'auc': roc_auc_score(y_test_fold, test_proba)
        })
    
    race_df = pd.DataFrame(race_scores)
    print(f"\nãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†å‰²çµæœ:")
    print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {race_df['accuracy'].mean():.3f} Â± {race_df['accuracy'].std():.3f}")
    print(f"ãƒ†ã‚¹ãƒˆAUC: {race_df['auc'].mean():.3f} Â± {race_df['auc'].std():.3f}")
    
    return group_scores, race_scores

group_scores, race_scores = group_cross_validation(df, X, y)
```

## é«˜åº¦ãªè©•ä¾¡æ‰‹æ³•

### 1. ãƒã‚¹ãƒ†ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

```python
def nested_cross_validation(X, y, groups):
    """ãƒã‚¹ãƒ†ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("\n=== ãƒã‚¹ãƒ†ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ ===")
    
    from sklearn.model_selection import GridSearchCV
    
    # å¤–å´ã®CVï¼ˆæ€§èƒ½è©•ä¾¡ç”¨ï¼‰
    outer_cv = GroupKFold(n_splits=5)
    
    # å†…å´ã®CVï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ç”¨ï¼‰
    inner_cv = GroupKFold(n_splits=3)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, None],
        'min_samples_split': [2, 5, 10]
    }
    
    # ãƒã‚¹ãƒ†ãƒƒãƒ‰CVå®Ÿè¡Œ
    nested_scores = []
    best_params_list = []
    
    for i, (train_outer, test_outer) in enumerate(outer_cv.split(X, y, groups)):
        print(f"\nå¤–å´ Fold {i+1}:")
        
        X_train_outer = X.iloc[train_outer]
        X_test_outer = X.iloc[test_outer]
        y_train_outer = y.iloc[train_outer]
        y_test_outer = y.iloc[test_outer]
        groups_train_outer = groups.iloc[train_outer]
        
        # å†…å´CVã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
        grid_search = GridSearchCV(
            RandomForestClassifier(random_state=42),
            param_grid,
            cv=inner_cv,
            scoring='roc_auc',
            n_jobs=-1
        )
        
        grid_search.fit(X_train_outer, y_train_outer, groups=groups_train_outer)
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¤–å´ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’è©•ä¾¡
        best_model = grid_search.best_estimator_
        test_pred = best_model.predict(X_test_outer)
        test_proba = best_model.predict_proba(X_test_outer)[:, 1]
        
        fold_score = {
            'accuracy': accuracy_score(y_test_outer, test_pred),
            'auc': roc_auc_score(y_test_outer, test_proba),
            'precision': precision_score(y_test_outer, test_pred),
            'recall': recall_score(y_test_outer, test_pred),
            'f1': f1_score(y_test_outer, test_pred)
        }
        
        nested_scores.append(fold_score)
        best_params_list.append(grid_search.best_params_)
        
        print(f"  æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
        print(f"  å†…å´CVæœ€é«˜ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.3f}")
        print(f"  å¤–å´ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢: {fold_score['auc']:.3f}")
    
    # çµæœã¾ã¨ã‚
    nested_df = pd.DataFrame(nested_scores)
    
    print(f"\n=== ãƒã‚¹ãƒ†ãƒƒãƒ‰CVæœ€çµ‚çµæœ ===")
    for metric in ['accuracy', 'auc', 'precision', 'recall', 'f1']:
        mean_score = nested_df[metric].mean()
        std_score = nested_df[metric].std()
        print(f"{metric.upper()}: {mean_score:.3f} Â± {std_score:.3f}")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®‰å®šæ€§ç¢ºèª
    param_stability = pd.DataFrame(best_params_list)
    print(f"\n=== ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠã®å®‰å®šæ€§ ===")
    for param in param_stability.columns:
        print(f"{param}: {param_stability[param].value_counts()}")
    
    return nested_scores, best_params_list

nested_scores, best_params = nested_cross_validation(X, y, df['race_id'])
```

### 2. å­¦ç¿’æ›²ç·šã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·š

```python
def plot_learning_validation_curves(X, y, groups):
    """å­¦ç¿’æ›²ç·šã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·šã®æç”»"""
    
    print("\n=== å­¦ç¿’æ›²ç·šãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·šåˆ†æ ===")
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    cv = GroupKFold(n_splits=5)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. å­¦ç¿’æ›²ç·š
    train_sizes = np.linspace(0.1, 1.0, 10)
    train_sizes_abs, train_scores, val_scores = learning_curve(
        model, X, y, groups=groups, cv=cv, 
        train_sizes=train_sizes, scoring='roc_auc',
        n_jobs=-1, random_state=42
    )
    
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    axes[0,0].plot(train_sizes_abs, train_mean, 'o-', label='è¨“ç·´ã‚¹ã‚³ã‚¢', alpha=0.8)
    axes[0,0].fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2)
    axes[0,0].plot(train_sizes_abs, val_mean, 'o-', label='æ¤œè¨¼ã‚¹ã‚³ã‚¢', alpha=0.8)
    axes[0,0].fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2)
    axes[0,0].set_xlabel('è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°')
    axes[0,0].set_ylabel('AUC Score')
    axes[0,0].set_title('å­¦ç¿’æ›²ç·š')
    axes[0,0].legend()
    axes[0,0].grid(True)
    
    # 2. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·šï¼ˆn_estimatorsï¼‰
    param_range = [10, 50, 100, 200, 300, 500]
    train_scores_val, val_scores_val = validation_curve(
        RandomForestClassifier(random_state=42), X, y, groups=groups,
        param_name='n_estimators', param_range=param_range,
        cv=cv, scoring='roc_auc', n_jobs=-1
    )
    
    train_mean_val = train_scores_val.mean(axis=1)
    train_std_val = train_scores_val.std(axis=1)
    val_mean_val = val_scores_val.mean(axis=1)
    val_std_val = val_scores_val.std(axis=1)
    
    axes[0,1].plot(param_range, train_mean_val, 'o-', label='è¨“ç·´ã‚¹ã‚³ã‚¢', alpha=0.8)
    axes[0,1].fill_between(param_range, train_mean_val - train_std_val, train_mean_val + train_std_val, alpha=0.2)
    axes[0,1].plot(param_range, val_mean_val, 'o-', label='æ¤œè¨¼ã‚¹ã‚³ã‚¢', alpha=0.8)
    axes[0,1].fill_between(param_range, val_mean_val - val_std_val, val_mean_val + val_std_val, alpha=0.2)
    axes[0,1].set_xlabel('n_estimators')
    axes[0,1].set_ylabel('AUC Score')
    axes[0,1].set_title('ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·š (n_estimators)')
    axes[0,1].legend()
    axes[0,1].grid(True)
    
    # 3. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·šï¼ˆmax_depthï¼‰
    param_range_depth = [3, 5, 7, 10, 15, None]
    param_range_depth_numeric = [3, 5, 7, 10, 15, 20]  # Noneã‚’20ã«ç½®æ›
    
    train_scores_depth, val_scores_depth = validation_curve(
        RandomForestClassifier(n_estimators=100, random_state=42), X, y, groups=groups,
        param_name='max_depth', param_range=param_range_depth,
        cv=cv, scoring='roc_auc', n_jobs=-1
    )
    
    train_mean_depth = train_scores_depth.mean(axis=1)
    train_std_depth = train_scores_depth.std(axis=1)
    val_mean_depth = val_scores_depth.mean(axis=1)
    val_std_depth = val_scores_depth.std(axis=1)
    
    axes[1,0].plot(param_range_depth_numeric, train_mean_depth, 'o-', label='è¨“ç·´ã‚¹ã‚³ã‚¢', alpha=0.8)
    axes[1,0].fill_between(param_range_depth_numeric, train_mean_depth - train_std_depth, train_mean_depth + train_std_depth, alpha=0.2)
    axes[1,0].plot(param_range_depth_numeric, val_mean_depth, 'o-', label='æ¤œè¨¼ã‚¹ã‚³ã‚¢', alpha=0.8)
    axes[1,0].fill_between(param_range_depth_numeric, val_mean_depth - val_std_depth, val_mean_depth + val_std_depth, alpha=0.2)
    axes[1,0].set_xlabel('max_depth (None=20)')
    axes[1,0].set_ylabel('AUC Score')
    axes[1,0].set_title('ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·š (max_depth)')
    axes[1,0].legend()
    axes[1,0].grid(True)
    
    # 4. éå­¦ç¿’ã®æ¤œå‡º
    overfitting_scores = []
    for i in range(len(train_sizes_abs)):
        overfitting = train_mean[i] - val_mean[i]
        overfitting_scores.append(overfitting)
    
    axes[1,1].plot(train_sizes_abs, overfitting_scores, 'ro-', alpha=0.8)
    axes[1,1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
    axes[1,1].set_xlabel('è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°')
    axes[1,1].set_ylabel('éå­¦ç¿’åº¦ (è¨“ç·´ã‚¹ã‚³ã‚¢ - æ¤œè¨¼ã‚¹ã‚³ã‚¢)')
    axes[1,1].set_title('éå­¦ç¿’ã®æ¤œå‡º')
    axes[1,1].grid(True)
    
    plt.tight_layout()
    plt.show()
    
    # éå­¦ç¿’ã®æ•°å€¤çš„è©•ä¾¡
    final_overfitting = train_mean[-1] - val_mean[-1]
    print(f"æœ€çµ‚çš„ãªéå­¦ç¿’åº¦: {final_overfitting:.4f}")
    
    if final_overfitting > 0.05:
        print("âš ï¸  éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    elif final_overfitting < -0.05:
        print("âš ï¸  å­¦ç¿’ä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™") 
    else:
        print("âœ… é©åˆ‡ãªå­¦ç¿’çŠ¶æ…‹ã§ã™")

plot_learning_validation_curves(X, y, df['race_id'])
```

## å®Ÿç”¨çš„ãªè©•ä¾¡æŒ‡æ¨™

### 1. ç«¶é¦¬ç‰¹æœ‰ã®è©•ä¾¡æŒ‡æ¨™

```python
def horse_racing_specific_metrics(df, X, y, groups):
    """ç«¶é¦¬ç‰¹æœ‰ã®è©•ä¾¡æŒ‡æ¨™"""
    
    print("\n=== ç«¶é¦¬ç‰¹æœ‰ã®è©•ä¾¡æŒ‡æ¨™ ===")
    
    from sklearn.model_selection import cross_val_predict
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    cv = GroupKFold(n_splits=5)
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®äºˆæ¸¬
    y_pred_proba = cross_val_predict(model, X, y, groups=groups, cv=cv, method='predict_proba')[:, 1]
    y_pred = cross_val_predict(model, X, y, groups=groups, cv=cv, method='predict')
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«äºˆæ¸¬çµæœã‚’è¿½åŠ 
    df_with_pred = df.copy()
    df_with_pred['pred_proba'] = y_pred_proba
    df_with_pred['pred_winner'] = y_pred
    
    # 1. ãƒ¬ãƒ¼ã‚¹å˜ä½ã§ã®çš„ä¸­ç‡
    def calculate_race_accuracy(race_group):
        """å„ãƒ¬ãƒ¼ã‚¹ã§ã®äºˆæƒ³çš„ä¸­ç‡"""
        if len(race_group) == 0:
            return np.nan
        
        # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„é¦¬ã‚’äºˆæƒ³
        predicted_winner_idx = race_group['pred_proba'].idxmax()
        actual_winner_idx = race_group[race_group['is_winner'] == 1].index
        
        if len(actual_winner_idx) == 0:
            return np.nan
        
        return 1 if predicted_winner_idx in actual_winner_idx.values else 0
    
    race_accuracy = df_with_pred.groupby('race_id').apply(calculate_race_accuracy)
    race_hit_rate = race_accuracy.mean()
    
    print(f"ãƒ¬ãƒ¼ã‚¹å˜ä½çš„ä¸­ç‡: {race_hit_rate:.3f}")
    
    # 2. äººæ°—åˆ¥çš„ä¸­ç‡
    popularity_accuracy = []
    for pop in range(1, 6):  # 1-5ç•ªäººæ°—
        pop_mask = df_with_pred['popularity'] == pop
        if pop_mask.sum() > 0:
            pop_acc = accuracy_score(df_with_pred[pop_mask]['is_winner'], 
                                   df_with_pred[pop_mask]['pred_winner'])
            popularity_accuracy.append((pop, pop_acc))
            print(f"{pop}ç•ªäººæ°—çš„ä¸­ç‡: {pop_acc:.3f}")
    
    # 3. å›åç‡è¨ˆç®—
    def calculate_return_rate(df_pred):
        """å›åç‡è¨ˆç®—"""
        total_bet = 0
        total_return = 0
        
        for race_id, race_group in df_pred.groupby('race_id'):
            if len(race_group) == 0:
                continue
            
            # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„é¦¬ã«è³­ã‘ã‚‹
            predicted_winner = race_group.loc[race_group['pred_proba'].idxmax()]
            
            total_bet += 100  # 100å††è³­ã‘
            
            # çš„ä¸­ã—ãŸå ´åˆã®æ‰•ã„æˆ»ã—
            if predicted_winner['is_winner'] == 1:
                total_return += predicted_winner['odds'] * 100
        
        return total_return / total_bet if total_bet > 0 else 0
    
    return_rate = calculate_return_rate(df_with_pred)
    print(f"å›åç‡: {return_rate:.3f} ({return_rate*100:.1f}%)")
    
    # 4. ä¿¡é ¼åº¦åˆ¥çš„ä¸­ç‡
    confidence_bins = [0, 0.1, 0.2, 0.3, 0.5, 1.0]
    confidence_labels = ['0-10%', '10-20%', '20-30%', '30-50%', '50-100%']
    
    df_with_pred['confidence_bin'] = pd.cut(df_with_pred['pred_proba'], 
                                          bins=confidence_bins, 
                                          labels=confidence_labels)
    
    print(f"\nä¿¡é ¼åº¦åˆ¥çš„ä¸­ç‡:")
    for bin_label in confidence_labels:
        bin_mask = df_with_pred['confidence_bin'] == bin_label
        if bin_mask.sum() > 0:
            bin_acc = accuracy_score(df_with_pred[bin_mask]['is_winner'],
                                   df_with_pred[bin_mask]['pred_winner'])
            bin_count = bin_mask.sum()
            print(f"{bin_label}: {bin_acc:.3f} ({bin_count}ã‚µãƒ³ãƒ—ãƒ«)")
    
    # 5. å®Ÿç”¨çš„ãªè©•ä¾¡ã‚µãƒãƒªãƒ¼
    metrics_summary = {
        'traditional_accuracy': accuracy_score(y, y_pred),
        'auc_score': roc_auc_score(y, y_pred_proba),
        'race_hit_rate': race_hit_rate,
        'return_rate': return_rate,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred)
    }
    
    return metrics_summary, df_with_pred

horse_metrics, df_with_pred = horse_racing_specific_metrics(df, X, y, df['race_id'])
```

### 2. çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š

```python
def statistical_significance_testing(df, X, y, groups):
    """çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š"""
    
    print("\n=== çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š ===")
    
    # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ã®æ¯”è¼ƒ
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)
    }
    
    cv = GroupKFold(n_splits=5)
    
    # å„ãƒ¢ãƒ‡ãƒ«ã®CV ã‚¹ã‚³ã‚¢åé›†
    model_scores = {}
    for model_name, model in models.items():
        scores = cross_val_score(model, X, y, groups=groups, cv=cv, scoring='roc_auc')
        model_scores[model_name] = scores
        print(f"{model_name}: {scores.mean():.3f} Â± {scores.std():.3f}")
    
    # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºtæ¤œå®š
    model_names = list(model_scores.keys())
    print(f"\n=== ãƒšã‚¢ãƒ¯ã‚¤ã‚ºtæ¤œå®š (på€¤) ===")
    print(f"{'':20s}", end="")
    for name in model_names:
        print(f"{name[:15]:15s}", end="")
    print()
    
    for i, name1 in enumerate(model_names):
        print(f"{name1[:20]:20s}", end="")
        for j, name2 in enumerate(model_names):
            if i != j:
                t_stat, p_value = ttest_rel(model_scores[name1], model_scores[name2])
                significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else ""
                print(f"{p_value:.3f}{significance:3s}   ", end="")
            else:
                print("     -      ", end="")
        print()
    
    print("\n* p<0.05, ** p<0.01, *** p<0.001")
    
    # åŠ¹æœé‡ï¼ˆCohen's dï¼‰ã®è¨ˆç®—
    def cohens_d(group1, group2):
        """Cohen's dåŠ¹æœé‡"""
        n1, n2 = len(group1), len(group2)
        s1, s2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
        pooled_std = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))
        return (np.mean(group1) - np.mean(group2)) / pooled_std
    
    print(f"\n=== åŠ¹æœé‡ (Cohen's d) ===")
    for i, name1 in enumerate(model_names):
        for j, name2 in enumerate(model_names[i+1:], i+1):
            d = cohens_d(model_scores[name1], model_scores[name2])
            effect_size = "å¤§" if abs(d) > 0.8 else "ä¸­" if abs(d) > 0.5 else "å°" if abs(d) > 0.2 else "ãªã—"
            print(f"{name1} vs {name2}: d={d:.3f} ({effect_size})")
    
    # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆã§å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    
    score_data = [model_scores[name] for name in model_names]
    plt.boxplot(score_data, labels=model_names)
    plt.ylabel('AUC Score')
    plt.title('ãƒ¢ãƒ‡ãƒ«é–“ã®ã‚¹ã‚³ã‚¢åˆ†å¸ƒæ¯”è¼ƒ')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    return model_scores

model_scores = statistical_significance_testing(df, X, y, df['race_id'])
```

## ãƒã‚¤ã‚¢ã‚¹ã¨ãƒ•ã‚§ã‚¢ãƒã‚¹è©•ä¾¡

```python
def bias_and_fairness_evaluation(df, df_with_pred):
    """ãƒã‚¤ã‚¢ã‚¹ã¨ãƒ•ã‚§ã‚¢ãƒã‚¹ã®è©•ä¾¡"""
    
    print("\n=== ãƒã‚¤ã‚¢ã‚¹ãƒ»ãƒ•ã‚§ã‚¢ãƒã‚¹è©•ä¾¡ ===")
    
    # 1. æ€§åˆ¥ãƒã‚¤ã‚¢ã‚¹
    print("1. æ€§åˆ¥ã«ã‚ˆã‚‹äºˆæƒ³ãƒã‚¤ã‚¢ã‚¹:")
    for sex in df['sex'].unique():
        sex_mask = df['sex'] == sex
        if sex_mask.sum() > 100:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«æ•°
            sex_accuracy = accuracy_score(
                df_with_pred[sex_mask]['is_winner'],
                df_with_pred[sex_mask]['pred_winner']
            )
            sex_auc = roc_auc_score(
                df_with_pred[sex_mask]['is_winner'],
                df_with_pred[sex_mask]['pred_proba']
            )
            print(f"  {sex}: ç²¾åº¦={sex_accuracy:.3f}, AUC={sex_auc:.3f}")
    
    # 2. ç«¶é¦¬å ´ãƒã‚¤ã‚¢ã‚¹
    print("\n2. ç«¶é¦¬å ´ã«ã‚ˆã‚‹äºˆæƒ³ãƒã‚¤ã‚¢ã‚¹:")
    for track in df['track'].unique():
        track_mask = df['track'] == track
        if track_mask.sum() > 100:
            track_accuracy = accuracy_score(
                df_with_pred[track_mask]['is_winner'],
                df_with_pred[track_mask]['pred_winner']
            )
            track_auc = roc_auc_score(
                df_with_pred[track_mask]['is_winner'],
                df_with_pred[track_mask]['pred_proba']
            )
            print(f"  {track}: ç²¾åº¦={track_accuracy:.3f}, AUC={track_auc:.3f}")
    
    # 3. æ™‚æœŸãƒã‚¤ã‚¢ã‚¹
    print("\n3. æœˆåˆ¥äºˆæƒ³ãƒã‚¤ã‚¢ã‚¹:")
    monthly_bias = df_with_pred.groupby('month').apply(
        lambda x: {
            'accuracy': accuracy_score(x['is_winner'], x['pred_winner']),
            'auc': roc_auc_score(x['is_winner'], x['pred_proba']) if x['is_winner'].sum() > 0 else np.nan,
            'sample_count': len(x)
        }
    )
    
    for month, metrics in monthly_bias.items():
        if metrics['sample_count'] > 50:
            print(f"  {month}æœˆ: ç²¾åº¦={metrics['accuracy']:.3f}, AUC={metrics['auc']:.3f}")
    
    # 4. äºˆæƒ³ç¢ºç‡ã®æ ¡æ­£
    from sklearn.calibration import calibration_curve
    
    fraction_of_positives, mean_predicted_value = calibration_curve(
        df_with_pred['is_winner'], 
        df_with_pred['pred_proba'], 
        n_bins=10
    )
    
    plt.figure(figsize=(12, 5))
    
    # æ ¡æ­£ãƒ—ãƒ­ãƒƒãƒˆ
    plt.subplot(1, 2, 1)
    plt.plot([0, 1], [0, 1], 'k:', label='å®Œå…¨æ ¡æ­£')
    plt.plot(mean_predicted_value, fraction_of_positives, 's-', label='äºˆæƒ³ãƒ¢ãƒ‡ãƒ«')
    plt.xlabel('å¹³å‡äºˆæƒ³ç¢ºç‡')
    plt.ylabel('å®Ÿéš›ã®å‹ç‡')
    plt.title('ç¢ºç‡æ ¡æ­£ãƒ—ãƒ­ãƒƒãƒˆ')
    plt.legend()
    plt.grid(True)
    
    # äºˆæƒ³ç¢ºç‡åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    winners = df_with_pred[df_with_pred['is_winner'] == 1]['pred_proba']
    losers = df_with_pred[df_with_pred['is_winner'] == 0]['pred_proba']
    
    plt.hist(losers, bins=20, alpha=0.6, label='éå‹åˆ©é¦¬', density=True)
    plt.hist(winners, bins=20, alpha=0.6, label='å‹åˆ©é¦¬', density=True)
    plt.xlabel('äºˆæƒ³ç¢ºç‡')
    plt.ylabel('å¯†åº¦')
    plt.title('äºˆæƒ³ç¢ºç‡åˆ†å¸ƒ')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    # æ ¡æ­£èª¤å·®è¨ˆç®—
    calibration_error = np.mean(np.abs(fraction_of_positives - mean_predicted_value))
    print(f"\næ ¡æ­£èª¤å·®: {calibration_error:.4f}")
    
    if calibration_error < 0.05:
        print("âœ… äºˆæƒ³ç¢ºç‡ã¯è‰¯ãæ ¡æ­£ã•ã‚Œã¦ã„ã¾ã™")
    elif calibration_error < 0.1:
        print("âš ï¸  äºˆæƒ³ç¢ºç‡ã«ã‚„ã‚„æ ¡æ­£èª¤å·®ãŒã‚ã‚Šã¾ã™")
    else:
        print("âŒ äºˆæƒ³ç¢ºç‡ã®æ ¡æ­£ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")

bias_and_fairness_evaluation(df, df_with_pred)
```

## ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥ã®é¸æŠã‚¬ã‚¤ãƒ‰

```python
def cv_strategy_recommendation(df):
    """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥ã®æ¨å¥¨"""
    
    print("\n=== ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ  ===")
    
    data_characteristics = {
        'sample_size': len(df),
        'n_features': len([col for col in df.columns if col not in ['race_id', 'race_date', 'is_winner']]),
        'time_span_days': (df['race_date'].max() - df['race_date'].min()).days,
        'n_races': df['race_id'].nunique(),
        'n_tracks': df['track'].nunique() if 'track' in df.columns else 1,
        'class_imbalance': df['is_winner'].mean(),
        'has_groups': df['race_id'].nunique() < len(df),
        'is_time_series': df['race_date'].nunique() > 1
    }
    
    print("ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§:")
    print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {data_characteristics['sample_size']:,}")
    print(f"  ç‰¹å¾´é‡æ•°: {data_characteristics['n_features']}")
    print(f"  æœŸé–“: {data_characteristics['time_span_days']}æ—¥")
    print(f"  ãƒ¬ãƒ¼ã‚¹æ•°: {data_characteristics['n_races']:,}")
    print(f"  ç«¶é¦¬å ´æ•°: {data_characteristics['n_tracks']}")
    print(f"  ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡: {data_characteristics['class_imbalance']:.3f}")
    
    # æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ 
    recommendations = []
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹
    if data_characteristics['sample_size'] < 1000:
        recommendations.append("âš ï¸  ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ãŸã‚ã€Leave-One-Out CVã‚’æ¤œè¨")
    elif data_characteristics['sample_size'] > 100000:
        recommendations.append("ğŸ’¡ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã€è¨ˆç®—åŠ¹ç‡ã‚’è€ƒæ…®ã—ã¦Hold-outæ³•ã‚‚æ¤œè¨")
    
    # æ™‚ç³»åˆ—æ€§
    if data_characteristics['is_time_series']:
        if data_characteristics['time_span_days'] > 365:
            recommendations.append("ğŸ• é•·æœŸæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿: TimeSeriesSplitå¿…é ˆ")
        else:
            recommendations.append("ğŸ• çŸ­æœŸæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿: é€šå¸¸ã®CVå¾Œã«æ™‚ç³»åˆ—åˆ†å‰²ã§æ¤œè¨¼")
    
    # ã‚°ãƒ«ãƒ¼ãƒ—æ§‹é€ 
    if data_characteristics['has_groups']:
        recommendations.append("ğŸ‘¥ ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—æ§‹é€ ã‚ã‚Š: GroupKFoldæ¨å¥¨")
    
    # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡
    if data_characteristics['class_imbalance'] < 0.05 or data_characteristics['class_imbalance'] > 0.95:
        recommendations.append("âš–ï¸  æ¥µåº¦ã®ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡: StratifiedKFoldå¿…é ˆ")
    elif data_characteristics['class_imbalance'] < 0.2 or data_characteristics['class_imbalance'] > 0.8:
        recommendations.append("âš–ï¸  ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡: StratifiedKFoldæ¨å¥¨")
    
    # è¨ˆç®—è¤‡é›‘åº¦
    complexity_score = data_characteristics['sample_size'] * data_characteristics['n_features']
    if complexity_score > 10000000:  # 1000ä¸‡
        recommendations.append("âš¡ é«˜è¨ˆç®—è¤‡é›‘åº¦: 3-5 fold CVã§ååˆ†")
    
    print(f"\næ¨å¥¨äº‹é …:")
    for i, rec in enumerate(recommendations, 1):
        print(f"{i}. {rec}")
    
    # æœ€çµ‚æ¨å¥¨
    print(f"\nğŸ¯ ç·åˆæ¨å¥¨:")
    
    if (data_characteristics['is_time_series'] and 
        data_characteristics['has_groups'] and 
        data_characteristics['class_imbalance'] < 0.2):
        print("   æ™‚ç³»åˆ—ã‚°ãƒ«ãƒ¼ãƒ—åŒ–å±¤åŒ–ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
        print("   1. GroupKFold (race_id)")
        print("   2. TimeSeriesSplit ã§ã®æ¤œè¨¼")
        print("   3. Stratified sampling within groups")
        
    elif data_characteristics['is_time_series']:
        print("   æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
        print("   - TimeSeriesSplit (n_splits=5)")
        
    elif data_characteristics['has_groups']:
        print("   ã‚°ãƒ«ãƒ¼ãƒ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
        print("   - GroupKFold (race_id, n_splits=5)")
        
    elif data_characteristics['class_imbalance'] < 0.2:
        print("   å±¤åŒ–ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
        print("   - StratifiedKFold (n_splits=5-10)")
        
    else:
        print("   æ¨™æº–ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
        print("   - KFold (n_splits=5-10)")
    
    return data_characteristics, recommendations

cv_recommendations = cv_strategy_recommendation(df)
```

## å®Ÿé‹ç”¨ã§ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
class ModelPerformanceMonitor:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model, validation_method='time_series'):
        self.model = model
        self.validation_method = validation_method
        self.performance_history = []
        self.drift_threshold = 0.05  # æ€§èƒ½åŠ£åŒ–ã®é–¾å€¤
        
    def add_performance_record(self, date, y_true, y_pred, y_pred_proba):
        """æ€§èƒ½è¨˜éŒ²ã‚’è¿½åŠ """
        metrics = {
            'date': date,
            'accuracy': accuracy_score(y_true, y_pred),
            'auc': roc_auc_score(y_true, y_pred_proba),
            'precision': precision_score(y_true, y_pred),
            'recall': recall_score(y_true, y_pred),
            'f1': f1_score(y_true, y_pred),
            'n_samples': len(y_true)
        }
        
        self.performance_history.append(metrics)
    
    def detect_performance_drift(self, window_size=30):
        """æ€§èƒ½åŠ£åŒ–ã®æ¤œå‡º"""
        if len(self.performance_history) < window_size * 2:
            return False, "å±¥æ­´ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
        
        # æœ€æ–°æœŸé–“ã¨éå»æœŸé–“ã®æ¯”è¼ƒ
        recent_performance = np.mean([
            record['auc'] for record in self.performance_history[-window_size:]
        ])
        
        past_performance = np.mean([
            record['auc'] for record in self.performance_history[-window_size*2:-window_size]
        ])
        
        drift_magnitude = past_performance - recent_performance
        
        is_drift = drift_magnitude > self.drift_threshold
        
        return is_drift, f"åŠ£åŒ–åº¦: {drift_magnitude:.4f}"
    
    def plot_performance_history(self):
        """æ€§èƒ½å±¥æ­´ã®å¯è¦–åŒ–"""
        if not self.performance_history:
            print("æ€§èƒ½å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        df_history = pd.DataFrame(self.performance_history)
        df_history['date'] = pd.to_datetime(df_history['date'])
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # AUCæ¨ç§»
        axes[0,0].plot(df_history['date'], df_history['auc'], 'b-', alpha=0.7)
        axes[0,0].set_title('AUC Score æ¨ç§»')
        axes[0,0].set_ylabel('AUC')
        axes[0,0].grid(True)
        
        # ç²¾åº¦æ¨ç§»
        axes[0,1].plot(df_history['date'], df_history['accuracy'], 'g-', alpha=0.7)
        axes[0,1].set_title('Accuracy æ¨ç§»')
        axes[0,1].set_ylabel('Accuracy')
        axes[0,1].grid(True)
        
        # F1ã‚¹ã‚³ã‚¢æ¨ç§»
        axes[1,0].plot(df_history['date'], df_history['f1'], 'r-', alpha=0.7)
        axes[1,0].set_title('F1 Score æ¨ç§»')
        axes[1,0].set_ylabel('F1 Score')
        axes[1,0].grid(True)
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°æ¨ç§»
        axes[1,1].plot(df_history['date'], df_history['n_samples'], 'purple', alpha=0.7)
        axes[1,1].set_title('ã‚µãƒ³ãƒ—ãƒ«æ•°æ¨ç§»')
        axes[1,1].set_ylabel('Sample Count')
        axes[1,1].grid(True)
        
        plt.tight_layout()
        plt.show()
    
    def generate_report(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.performance_history:
            return "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
        
        df_history = pd.DataFrame(self.performance_history)
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        recent_avg = df_history.tail(10)['auc'].mean()
        overall_avg = df_history['auc'].mean()
        trend = "æ”¹å–„" if recent_avg > overall_avg else "æ‚ªåŒ–"
        
        # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
        is_drift, drift_msg = self.detect_performance_drift()
        
        report = f"""
=== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ ===
æœŸé–“: {df_history['date'].min()} - {df_history['date'].max()}
è©•ä¾¡å›æ•°: {len(self.performance_history)}

=== æ€§èƒ½æŒ‡æ¨™ ===
å…¨æœŸé–“å¹³å‡AUC: {overall_avg:.4f}
æœ€æ–°10å›å¹³å‡AUC: {recent_avg:.4f}
ãƒˆãƒ¬ãƒ³ãƒ‰: {trend} ({recent_avg - overall_avg:+.4f})

=== ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º ===
ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º: {'Yes' if is_drift else 'No'}
è©³ç´°: {drift_msg}

=== æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===
"""
        
        if is_drift:
            report += "- ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ã‚’æ¤œè¨\n- ç‰¹å¾´é‡ã®è¦‹ç›´ã—\n- ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç¢ºèª"
        else:
            report += "- ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã¯é©åˆ‡ã«æ©Ÿèƒ½\n- å®šæœŸãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ç¶™ç¶š"
        
        return report

# ä½¿ç”¨ä¾‹
monitor = ModelPerformanceMonitor(
    model=RandomForestClassifier(n_estimators=100, random_state=42)
)

# ã‚µãƒ³ãƒ—ãƒ«æ€§èƒ½ãƒ‡ãƒ¼ã‚¿
dates = pd.date_range('2024-01-01', periods=100, freq='D')
for date in dates:
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    n_samples = np.random.randint(50, 200)
    y_true = np.random.binomial(1, 0.1, n_samples)
    y_pred = np.random.binomial(1, 0.12, n_samples)  # è‹¥å¹²ã®åã‚Š
    y_pred_proba = np.random.uniform(0, 1, n_samples)
    
    monitor.add_performance_record(date, y_true, y_pred, y_pred_proba)

print(monitor.generate_report())
monitor.plot_performance_history()
```

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€ç«¶é¦¬äºˆæƒ³ã«ãŠã‘ã‚‹æ­£ç¢ºãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•ã‚’åŒ…æ‹¬çš„ã«è§£èª¬ã—ã¾ã—ãŸã€‚

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼š

1. **ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã®ç†è§£**:
   - æ™‚ç³»åˆ—ä¾å­˜æ€§
   - ã‚°ãƒ«ãƒ¼ãƒ—æ§‹é€ ï¼ˆãƒ¬ãƒ¼ã‚¹å˜ä½ï¼‰
   - ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡

2. **é©åˆ‡ãªCVæ‰‹æ³•ã®é¸æŠ**:
   - æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿: TimeSeriesSplit
   - ã‚°ãƒ«ãƒ¼ãƒ—æ§‹é€ : GroupKFold
   - ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿: StratifiedKFold

3. **é«˜åº¦ãªè©•ä¾¡æŠ€è¡“**:
   - ãƒã‚¹ãƒ†ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
   - å­¦ç¿’ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·š
   - çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š

4. **å®Ÿç”¨çš„è©•ä¾¡æŒ‡æ¨™**:
   - ãƒ¬ãƒ¼ã‚¹å˜ä½çš„ä¸­ç‡
   - å›åç‡
   - ç¢ºç‡æ ¡æ­£

5. **ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**:
   - æ€§èƒ½åŠ£åŒ–ã®æ¤œå‡º
   - ãƒ‰ãƒªãƒ•ãƒˆåˆ†æ
   - å®šæœŸçš„ãªå†è©•ä¾¡

### å®Ÿè·µçš„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼š

- **ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º**: 1000ã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸‹ãªã‚‰5-foldã€ãã‚Œä»¥ä¸Šãªã‚‰10-fold
- **è¨ˆç®—æ™‚é–“**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯3-foldã€ç²¾åº¦é‡è¦–ãªã‚‰10-fold
- **æ¥­å‹™è¦ä»¶**: å®Ÿé‹ç”¨ã§ã¯æ™‚ç³»åˆ—åˆ†å‰²å¿…é ˆ
- **çµ±è¨ˆçš„æ¤œå®š**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒæ™‚ã¯æœ‰æ„æ€§æ¤œå®šå®Ÿæ–½

é©åˆ‡ãªã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€éå­¦ç¿’ã‚’é˜²ãã€å®Ÿéš›ã®äºˆæƒ³ç²¾åº¦ã«ã‚ˆã‚Šè¿‘ã„è©•ä¾¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’ç†è§£ã—ã€ç›®çš„ã«å¿œã˜ãŸè©•ä¾¡æ‰‹æ³•ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒã€ä¿¡é ¼æ€§ã®é«˜ã„ç«¶é¦¬äºˆæƒ³ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã®éµã¨ãªã‚Šã¾ã™ã€‚

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"id": "1", "status": "completed", "content": "\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020\u3092\u78ba\u8a8d\u3057\u3066\u3001content/machine-learning/\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5b58\u5728\u78ba\u8a8d"}, {"id": "2", "status": "completed", "content": "\u8a18\u4e8b1\uff1a\u300cPython\u3067\u59cb\u3081\u308b\u7af6\u99ac\u30c7\u30fc\u30bf\u5206\u6790 - pandas\u57fa\u790e\u304b\u3089XGBoost\u307e\u3067\u300d\u3092\u4f5c\u6210\uff08slug: python-horse-racing-data-analysis\uff09"}, {"id": "3", "status": "completed", "content": "\u8a18\u4e8b2\uff1a\u300c\u5357\u95a2\u7af6\u99ac\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u91cf\u8a2d\u8a08 - 50\u500b\u306e\u5909\u6570\u3067\u7cbe\u5ea6\u5411\u4e0a\u300d\u3092\u4f5c\u6210\uff08slug: nankan-feature-engineering\uff09"}, {"id": "4", "status": "completed", "content": "\u8a18\u4e8b3\uff1a\u300cscikit-learn\u3067\u7af6\u99ac\u4e88\u60f3\u30e2\u30c7\u30eb\u69cb\u7bc9 - \u521d\u5fc3\u8005\u5411\u3051\u5b8c\u5168\u30ac\u30a4\u30c9\u300d\u3092\u4f5c\u6210\uff08slug: scikit-learn-horse-prediction\uff09"}, {"id": "5", "status": "completed", "content": "\u8a18\u4e8b4\uff1a\u300c\u7af6\u99ac\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u30c6\u30af\u30cb\u30c3\u30af - \u6b20\u640d\u5024\u304b\u3089\u5916\u308c\u5024\u307e\u3067\u300d\u3092\u4f5c\u6210\uff08slug: horse-data-preprocessing\uff09"}, {"id": "6", "status": "completed", "content": "\u8a18\u4e8b5\uff1a\u300c\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u4e88\u60f3\u7cbe\u5ea6\u3092\u6b63\u3057\u304f\u8a55\u4fa1\u3059\u308b\u65b9\u6cd5\u300d\u3092\u4f5c\u6210\uff08slug: cross-validation-horse-prediction\uff09"}]