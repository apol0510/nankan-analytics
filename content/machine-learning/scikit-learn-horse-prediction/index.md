---
title: "scikit-learnã§ç«¶é¦¬äºˆæƒ³ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ - åˆå¿ƒè€…å‘ã‘å®Œå…¨ã‚¬ã‚¤ãƒ‰"
date: 2025-08-07T11:00:00+09:00
description: "scikit-learnã‚’ä½¿ã£ã¦ç«¶é¦¬äºˆæƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€ã‹ã‚‰æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’ã€åˆå¿ƒè€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãè§£èª¬ã€‚ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‹ã‚‰å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒã€ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¾ã§å®Ÿè·µçš„ã«å­¦ã³ã¾ã™ã€‚"
categories: ["æ©Ÿæ¢°å­¦ç¿’", "scikit-learn"]
tags: ["scikit-learn", "ç«¶é¦¬äºˆæƒ³", "æ©Ÿæ¢°å­¦ç¿’", "Python", "åˆ†é¡å•é¡Œ", "åˆå¿ƒè€…å‘ã‘"]
slug: "scikit-learn-horse-prediction"
---

## ã¯ã˜ã‚ã«

scikit-learnã¯æ©Ÿæ¢°å­¦ç¿’ã®å…¥é–€ã«æœ€é©ãªPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€ç«¶é¦¬äºˆæƒ³ãƒ¢ãƒ‡ãƒ«ã‚’é¡Œæã«ã€scikit-learnã‚’ä½¿ã£ãŸæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…¨å·¥ç¨‹ã‚’åˆå¿ƒè€…å‘ã‘ã«è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚

## scikit-learnã¨ã¯

scikit-learnã¯ã€Pythonã§æœ€ã‚‚äººæ°—ã®é«˜ã„æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¸€ã¤ã§ã™ï¼š

- **è±Šå¯Œãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: åˆ†é¡ã€å›å¸°ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
- **çµ±ä¸€ã•ã‚ŒãŸAPI**: ã©ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚‚åŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- **å……å®Ÿã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: åˆå¿ƒè€…ã«å„ªã—ã„èª¬æ˜
- **å®Ÿç”¨çš„ãªãƒ„ãƒ¼ãƒ«**: å‰å‡¦ç†ã€è©•ä¾¡æŒ‡æ¨™ã€å¯è¦–åŒ–

## ç’°å¢ƒæ§‹ç¯‰

```python
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install scikit-learn pandas numpy matplotlib seaborn jupyter

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

# scikit-learnã‹ã‚‰ã®ä¸»è¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

# è­¦å‘Šã®éè¡¨ç¤º
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªå¯¾å¿œ
plt.rcParams['figure.figsize'] = (10, 6)
print("ç’°å¢ƒæ§‹ç¯‰å®Œäº†")
```

## ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

### ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

```python
# ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
np.random.seed(42)
n_races = 1500
n_horses_per_race = 12

data = []
for race_id in range(n_races):
    # ãƒ¬ãƒ¼ã‚¹æ¡ä»¶
    distance = np.random.choice([1200, 1400, 1600, 1800, 2000, 2400])
    track_condition = np.random.choice(['è‰¯', 'ç¨é‡', 'é‡', 'ä¸è‰¯'], p=[0.6, 0.25, 0.1, 0.05])
    race_class = np.random.choice(['æ–°é¦¬', 'æœªå‹åˆ©', '1å‹ã‚¯ãƒ©ã‚¹', '2å‹ã‚¯ãƒ©ã‚¹', '3å‹ã‚¯ãƒ©ã‚¹', 'ã‚ªãƒ¼ãƒ—ãƒ³'])
    
    for horse_id in range(n_horses_per_race):
        # é¦¬ã®åŸºæœ¬æƒ…å ±
        age = np.random.randint(3, 8)
        sex = np.random.choice(['ç‰¡', 'ç‰', 'ã‚»'], p=[0.5, 0.4, 0.1])
        weight = np.random.randint(440, 540)
        
        # ã‚ªãƒƒã‚ºï¼ˆäººæ°—é †ã«é€£å‹•ï¼‰
        popularity = horse_id + 1
        base_odds = 1.5 + (popularity - 1) * 2
        odds = base_odds + np.random.normal(0, base_odds * 0.3)
        odds = max(1.1, odds)  # æœ€ä½ã‚ªãƒƒã‚º
        
        # ç€é †ï¼ˆäººæ°—é¦¬ã»ã©ä¸Šä½ã«æ¥ã‚„ã™ã„ï¼‰
        prob_weights = np.array([10 - i for i in range(n_horses_per_race)])
        prob_weights = prob_weights / prob_weights.sum()
        finish_positions = list(range(1, n_horses_per_race + 1))
        
        data.append({
            'race_id': race_id,
            'horse_id': f'horse_{race_id}_{horse_id}',
            'popularity': popularity,
            'odds': odds,
            'age': age,
            'sex': sex,
            'weight': weight,
            'distance': distance,
            'track_condition': track_condition,
            'race_class': race_class,
            'jockey_win_rate': np.random.uniform(0.05, 0.25),  # é¨æ‰‹å‹ç‡
            'trainer_win_rate': np.random.uniform(0.05, 0.20),  # èª¿æ•™å¸«å‹ç‡
        })

# ç€é †ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰²ã‚ŠæŒ¯ã‚Šï¼ˆäººæ°—ã‚’è€ƒæ…®ï¼‰
for race_id in range(n_races):
    race_horses = [d for d in data if d['race_id'] == race_id]
    # äººæ°—é †ã«ç€é †ã‚’æ±ºå®šï¼ˆç¢ºç‡çš„ï¼‰
    finish_positions = list(range(1, len(race_horses) + 1))
    
    for i, horse in enumerate(race_horses):
        # äººæ°—é¦¬ã»ã©ä¸Šä½ã«æ¥ã‚„ã™ã„ç¢ºç‡åˆ†å¸ƒ
        remaining_positions = [p for p in finish_positions if p not in [h.get('finish_position') for h in race_horses[:i]]]
        if remaining_positions:
            # äººæ°—ã«åŸºã¥ãé‡ã¿ä»˜ã‘
            pop = horse['popularity']
            weights = [1.0 / (p + pop * 0.5) for p in remaining_positions]
            weights = np.array(weights) / sum(weights)
            
            finish_pos = np.random.choice(remaining_positions, p=weights)
            horse['finish_position'] = finish_pos

df = pd.DataFrame(data)
print(f"ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {df.shape}")
print("\nåŸºæœ¬çµ±è¨ˆ:")
print(df.describe())
```

### ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã¨å¯è¦–åŒ–

```python
# ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±
print("ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±:")
print(df.info())

# ç›®çš„å¤‰æ•°ã®ç¢ºèª
df['is_winner'] = (df['finish_position'] == 1).astype(int)
print(f"\nå‹åˆ©é¦¬ã®å‰²åˆ: {df['is_winner'].mean():.3f}")

# åŸºæœ¬çš„ãªå¯è¦–åŒ–
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. ç€é †åˆ†å¸ƒ
axes[0, 0].hist(df['finish_position'], bins=12, edgecolor='black', alpha=0.7)
axes[0, 0].set_title('ç€é †åˆ†å¸ƒ')
axes[0, 0].set_xlabel('ç€é †')
axes[0, 0].set_ylabel('é »åº¦')

# 2. ã‚ªãƒƒã‚ºåˆ†å¸ƒ
axes[0, 1].hist(df['odds'], bins=30, edgecolor='black', alpha=0.7)
axes[0, 1].set_title('ã‚ªãƒƒã‚ºåˆ†å¸ƒ')
axes[0, 1].set_xlabel('ã‚ªãƒƒã‚º')
axes[0, 1].set_ylabel('é »åº¦')

# 3. äººæ°—ã¨ç€é †ã®é–¢ä¿‚
pop_finish = df.groupby('popularity')['finish_position'].mean()
axes[0, 2].bar(pop_finish.index, pop_finish.values)
axes[0, 2].set_title('äººæ°—åˆ¥å¹³å‡ç€é †')
axes[0, 2].set_xlabel('äººæ°—')
axes[0, 2].set_ylabel('å¹³å‡ç€é †')

# 4. å¹´é½¢åˆ†å¸ƒ
axes[1, 0].hist(df['age'], bins=6, edgecolor='black', alpha=0.7)
axes[1, 0].set_title('é¦¬é½¢åˆ†å¸ƒ')
axes[1, 0].set_xlabel('å¹´é½¢')
axes[1, 0].set_ylabel('é »åº¦')

# 5. æ€§åˆ¥åˆ¥å‹ç‡
sex_win_rate = df.groupby('sex')['is_winner'].mean()
axes[1, 1].bar(sex_win_rate.index, sex_win_rate.values)
axes[1, 1].set_title('æ€§åˆ¥å‹ç‡')
axes[1, 1].set_xlabel('æ€§åˆ¥')
axes[1, 1].set_ylabel('å‹ç‡')

# 6. ã‚ªãƒƒã‚ºã¨ç€é †ã®æ•£å¸ƒå›³
axes[1, 2].scatter(df['odds'], df['finish_position'], alpha=0.5)
axes[1, 2].set_title('ã‚ªãƒƒã‚ºã¨ç€é †ã®é–¢ä¿‚')
axes[1, 2].set_xlabel('ã‚ªãƒƒã‚º')
axes[1, 2].set_ylabel('ç€é †')

plt.tight_layout()
plt.show()
```

## ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†

### 1. æ¬ æå€¤ã®å‡¦ç†

```python
# æ¬ æå€¤ã®ç¢ºèª
print("æ¬ æå€¤ã®ç¢ºèª:")
print(df.isnull().sum())

# æ¬ æå€¤ãŒã‚ã‚‹å ´åˆã®å‡¦ç†ä¾‹
# df['feature_name'] = df['feature_name'].fillna(df['feature_name'].median())
```

### 2. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®å‡¦ç†

```python
# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
categorical_columns = ['sex', 'track_condition', 'race_class']

# ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
label_encoders = {}
df_encoded = df.copy()

for col in categorical_columns:
    le = LabelEncoder()
    df_encoded[f'{col}_encoded'] = le.fit_transform(df_encoded[col])
    label_encoders[col] = le
    print(f"{col}: {le.classes_}")

# ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ä¾‹
df_onehot = pd.get_dummies(df[categorical_columns], prefix=categorical_columns)
print(f"\nãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¾Œã®ç‰¹å¾´é‡æ•°: {df_onehot.shape[1]}")
print(df_onehot.head())
```

### 3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

```python
def create_features(df):
    """ç«¶é¦¬äºˆæƒ³ç”¨ã®ç‰¹å¾´é‡ã‚’ä½œæˆ"""
    features = df.copy()
    
    # 1. ã‚ªãƒƒã‚ºé–¢é€£ç‰¹å¾´é‡
    features['odds_log'] = np.log(features['odds'])
    features['odds_sqrt'] = np.sqrt(features['odds'])
    features['is_favorite'] = (features['popularity'] == 1).astype(int)
    features['is_popular'] = (features['popularity'] <= 3).astype(int)
    
    # 2. ãƒ¬ãƒ¼ã‚¹å†…ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    features['odds_rank'] = features.groupby('race_id')['odds'].rank()
    features['weight_rank'] = features.groupby('race_id')['weight'].rank()
    features['age_rank'] = features.groupby('race_id')['age'].rank()
    
    # 3. è·é›¢é©æ€§
    features['is_sprint'] = (features['distance'] <= 1400).astype(int)
    features['is_mile'] = ((features['distance'] > 1400) & (features['distance'] <= 1800)).astype(int)
    features['is_long'] = (features['distance'] > 1800).astype(int)
    
    # 4. é¦¬ã®ç‰¹å¾´
    features['is_young'] = (features['age'] <= 4).astype(int)
    features['is_female'] = (features['sex'] == 'ç‰').astype(int)
    features['weight_per_age'] = features['weight'] / features['age']
    
    # 5. ç›¸äº’ä½œç”¨ç‰¹å¾´é‡
    features['jockey_trainer_interaction'] = features['jockey_win_rate'] * features['trainer_win_rate']
    features['odds_popularity_gap'] = features['odds'] - features['popularity']
    
    return features

df_features = create_features(df_encoded)
print("ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†")
print(f"æ–°ã—ã„ç‰¹å¾´é‡æ•°: {df_features.shape[1] - df.shape[1]}")
```

### 4. ç‰¹å¾´é‡ã®æ¨™æº–åŒ–

```python
# æ•°å€¤ç‰¹å¾´é‡ã®é¸æŠ
numeric_features = [
    'odds', 'odds_log', 'odds_sqrt', 'age', 'weight', 'popularity',
    'jockey_win_rate', 'trainer_win_rate', 'odds_rank', 'weight_rank',
    'age_rank', 'weight_per_age', 'jockey_trainer_interaction', 'odds_popularity_gap'
]

# StandardScalerã«ã‚ˆã‚‹æ¨™æº–åŒ–
scaler = StandardScaler()
df_scaled = df_features.copy()
df_scaled[numeric_features] = scaler.fit_transform(df_features[numeric_features])

print("æ¨™æº–åŒ–å‰å¾Œã®æ¯”è¼ƒï¼ˆæœ€åˆã®3è¡Œï¼‰:")
print("æ¨™æº–åŒ–å‰:")
print(df_features[numeric_features].head(3))
print("\næ¨™æº–åŒ–å¾Œ:")
print(df_scaled[numeric_features].head(3))
```

## ç‰¹å¾´é‡é¸æŠ

```python
# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æº–å‚™
feature_columns = numeric_features + [f'{col}_encoded' for col in categorical_columns] + [
    'is_favorite', 'is_popular', 'is_sprint', 'is_mile', 'is_long', 
    'is_young', 'is_female'
]

X = df_scaled[feature_columns]
y = df_scaled['is_winner']

print(f"ç‰¹å¾´é‡æ•°: {X.shape[1]}")
print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")

# çµ±è¨ˆçš„ç‰¹å¾´é‡é¸æŠ
selector = SelectKBest(score_func=f_classif, k=15)
X_selected = selector.fit_transform(X, y)

# é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®ç¢ºèª
selected_indices = selector.get_support(indices=True)
selected_features = [feature_columns[i] for i in selected_indices]
feature_scores = selector.scores_[selected_indices]

feature_importance_df = pd.DataFrame({
    'feature': selected_features,
    'score': feature_scores
}).sort_values('score', ascending=False)

print("\né¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆé‡è¦åº¦é †ï¼‰:")
print(feature_importance_df)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 8))
plt.barh(range(len(feature_importance_df)), feature_importance_df['score'][::-1])
plt.yticks(range(len(feature_importance_df)), feature_importance_df['feature'][::-1])
plt.xlabel('Fçµ±è¨ˆé‡ã‚¹ã‚³ã‚¢')
plt.title('ç‰¹å¾´é‡é¸æŠçµæœ')
plt.tight_layout()
plt.show()
```

## ãƒ‡ãƒ¼ã‚¿åˆ†å‰²

```python
# è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
# ã¾ãšè¨“ç·´+æ¤œè¨¼ã¨ãƒ†ã‚¹ãƒˆã«åˆ†å‰²
X_temp, X_test, y_temp, y_test = train_test_split(
    X_selected, y, test_size=0.2, random_state=42, stratify=y
)

# è¨“ç·´ã¨æ¤œè¨¼ã«åˆ†å‰²
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)

print("ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0]} ã‚µãƒ³ãƒ—ãƒ«")
print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {X_val.shape[0]} ã‚µãƒ³ãƒ—ãƒ«") 
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]} ã‚µãƒ³ãƒ—ãƒ«")

print("\nå„ã‚»ãƒƒãƒˆã®å‹ç‡:")
print(f"è¨“ç·´: {y_train.mean():.3f}")
print(f"æ¤œè¨¼: {y_val.mean():.3f}")
print(f"ãƒ†ã‚¹ãƒˆ: {y_test.mean():.3f}")
```

## è¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ

```python
# æ¯”è¼ƒã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
algorithms = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(random_state=42, probability=True),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
    'Naive Bayes': GaussianNB()
}

# ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½æ¯”è¼ƒ
results = {}

print("å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½æ¯”è¼ƒ:")
print("-" * 80)
print(f"{'Algorithm':<20} {'Train Acc':<10} {'Val Acc':<10} {'Val AUC':<10} {'Time(s)':<10}")
print("-" * 80)

import time

for name, algorithm in algorithms.items():
    # å­¦ç¿’æ™‚é–“æ¸¬å®š
    start_time = time.time()
    
    # å­¦ç¿’
    algorithm.fit(X_train, y_train)
    
    # äºˆæ¸¬
    train_pred = algorithm.predict(X_train)
    val_pred = algorithm.predict(X_val)
    
    # ç¢ºç‡äºˆæ¸¬ï¼ˆAUCè¨ˆç®—ç”¨ï¼‰
    if hasattr(algorithm, 'predict_proba'):
        val_proba = algorithm.predict_proba(X_val)[:, 1]
    else:
        val_proba = algorithm.decision_function(X_val)
    
    end_time = time.time()
    
    # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
    train_acc = accuracy_score(y_train, train_pred)
    val_acc = accuracy_score(y_val, val_pred)
    val_auc = roc_auc_score(y_val, val_proba)
    training_time = end_time - start_time
    
    results[name] = {
        'model': algorithm,
        'train_acc': train_acc,
        'val_acc': val_acc,
        'val_auc': val_auc,
        'time': training_time
    }
    
    print(f"{name:<20} {train_acc:<10.3f} {val_acc:<10.3f} {val_auc:<10.3f} {training_time:<10.3f}")

print("-" * 80)
```

## è©³ç´°ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡

```python
# æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
best_model_name = max(results.keys(), key=lambda k: results[k]['val_auc'])
best_model = results[best_model_name]['model']

print(f"æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_model_name}")
print(f"æ¤œè¨¼AUC: {results[best_model_name]['val_auc']:.3f}")

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚è©•ä¾¡
test_pred = best_model.predict(X_test)
test_proba = best_model.predict_proba(X_test)[:, 1]

test_acc = accuracy_score(y_test, test_pred)
test_auc = roc_auc_score(y_test, test_proba)

print(f"\næœ€çµ‚ãƒ†ã‚¹ãƒˆçµæœ:")
print(f"ç²¾åº¦: {test_acc:.3f}")
print(f"AUC: {test_auc:.3f}")

# è©³ç´°ãªè©•ä¾¡çµæœ
print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, test_pred))

# æ··åŒè¡Œåˆ—
cm = confusion_matrix(y_test, test_pred)
plt.figure(figsize=(15, 5))

# 1. æ··åŒè¡Œåˆ—
plt.subplot(1, 3, 1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('æ··åŒè¡Œåˆ—')
plt.ylabel('å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«')
plt.xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«')

# 2. ROCæ›²ç·š
fpr, tpr, thresholds = roc_curve(y_test, test_proba)
plt.subplot(1, 3, 2)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {test_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROCæ›²ç·š')
plt.legend()

# 3. äºˆæ¸¬ç¢ºç‡ã®åˆ†å¸ƒ
plt.subplot(1, 3, 3)
plt.hist(test_proba[y_test == 0], alpha=0.5, label='éå‹åˆ©', bins=20, density=True)
plt.hist(test_proba[y_test == 1], alpha=0.5, label='å‹åˆ©', bins=20, density=True)
plt.xlabel('äºˆæ¸¬ç¢ºç‡')
plt.ylabel('å¯†åº¦')
plt.title('äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ')
plt.legend()

plt.tight_layout()
plt.show()
```

## ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

```python
# æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    base_model = RandomForestClassifier(random_state=42)
    
elif best_model_name == 'Gradient Boosting':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.05, 0.1, 0.2],
        'max_depth': [3, 5, 7],
        'subsample': [0.8, 1.0]
    }
    base_model = GradientBoostingClassifier(random_state=42)
    
elif best_model_name == 'Logistic Regression':
    param_grid = {
        'C': [0.01, 0.1, 1, 10, 100],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear']
    }
    base_model = LogisticRegression(random_state=42, max_iter=1000)
    
else:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
    param_grid = {}
    base_model = best_model

if param_grid:
    print(f"{best_model_name}ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚’å®Ÿè¡Œä¸­...")
    
    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
    grid_search = GridSearchCV(
        base_model, 
        param_grid, 
        cv=5, 
        scoring='roc_auc',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
    print(f"æœ€é«˜CV AUCã‚¹ã‚³ã‚¢: {grid_search.best_score_:.3f}")
    
    # æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§å†è©•ä¾¡
    optimized_model = grid_search.best_estimator_
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
    opt_test_pred = optimized_model.predict(X_test)
    opt_test_proba = optimized_model.predict_proba(X_test)[:, 1]
    
    opt_test_acc = accuracy_score(y_test, opt_test_pred)
    opt_test_auc = roc_auc_score(y_test, opt_test_proba)
    
    print(f"\næœ€é©åŒ–å¾Œã®ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"ç²¾åº¦: {opt_test_acc:.3f} (æ”¹å–„: {opt_test_acc - test_acc:+.3f})")
    print(f"AUC: {opt_test_auc:.3f} (æ”¹å–„: {opt_test_auc - test_auc:+.3f})")
    
    final_model = optimized_model
else:
    final_model = best_model
    print("ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
```

## ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ

```python
# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦
if hasattr(final_model, 'feature_importances_'):
    # Tree-based modelsã®å ´åˆ
    feature_importance = pd.DataFrame({
        'feature': selected_features,
        'importance': final_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    plt.figure(figsize=(12, 8))
    plt.barh(range(len(feature_importance)), feature_importance['importance'][::-1])
    plt.yticks(range(len(feature_importance)), feature_importance['feature'][::-1])
    plt.xlabel('ç‰¹å¾´é‡é‡è¦åº¦')
    plt.title(f'{best_model_name} - ç‰¹å¾´é‡é‡è¦åº¦')
    plt.tight_layout()
    plt.show()
    
    print("ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    print(feature_importance)

elif hasattr(final_model, 'coef_'):
    # Linear modelsã®å ´åˆ
    feature_coef = pd.DataFrame({
        'feature': selected_features,
        'coefficient': final_model.coef_[0],
        'abs_coefficient': np.abs(final_model.coef_[0])
    }).sort_values('abs_coefficient', ascending=False)
    
    plt.figure(figsize=(12, 8))
    colors = ['red' if x < 0 else 'blue' for x in feature_coef['coefficient'][::-1]]
    plt.barh(range(len(feature_coef)), feature_coef['coefficient'][::-1], color=colors)
    plt.yticks(range(len(feature_coef)), feature_coef['feature'][::-1])
    plt.xlabel('å›å¸°ä¿‚æ•°')
    plt.title(f'{best_model_name} - å›å¸°ä¿‚æ•°')
    plt.tight_layout()
    plt.show()
    
    print("å›å¸°ä¿‚æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    print(feature_coef)
```

## ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

```python
# ã‚ˆã‚Šå®‰å®šã—ãŸè©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
cv_scores_acc = cross_val_score(final_model, X_train, y_train, cv=5, scoring='accuracy')
cv_scores_auc = cross_val_score(final_model, X_train, y_train, cv=5, scoring='roc_auc')

print("5-Fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:")
print(f"ç²¾åº¦: {cv_scores_acc.mean():.3f} (+/- {cv_scores_acc.std() * 2:.3f})")
print(f"AUC: {cv_scores_auc.mean():.3f} (+/- {cv_scores_auc.std() * 2:.3f})")

# CVçµæœã®å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].bar(range(1, 6), cv_scores_acc)
axes[0].set_title('Cross-Validation Accuracy Scores')
axes[0].set_xlabel('Fold')
axes[0].set_ylabel('Accuracy')
axes[0].axhline(y=cv_scores_acc.mean(), color='red', linestyle='--', label='Mean')
axes[0].legend()

axes[1].bar(range(1, 6), cv_scores_auc)
axes[1].set_title('Cross-Validation AUC Scores')
axes[1].set_xlabel('Fold')
axes[1].set_ylabel('AUC')
axes[1].axhline(y=cv_scores_auc.mean(), color='red', linestyle='--', label='Mean')
axes[1].legend()

plt.tight_layout()
plt.show()
```

## å®Ÿç”¨çš„ãªäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ 

```python
class HorseRacingPredictor:
    """ç«¶é¦¬äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model, scaler, feature_selector, selected_features):
        self.model = model
        self.scaler = scaler
        self.feature_selector = feature_selector
        self.selected_features = selected_features
        
    def predict_race(self, race_data):
        """ãƒ¬ãƒ¼ã‚¹äºˆæƒ³ã‚’å®Ÿè¡Œ"""
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        race_features = create_features(race_data)
        
        # ç‰¹å¾´é‡é¸æŠ
        X_race = race_features[feature_columns]
        X_race_scaled = self.scaler.transform(X_race)
        X_race_selected = self.feature_selector.transform(X_race_scaled)
        
        # äºˆæ¸¬å®Ÿè¡Œ
        probabilities = self.model.predict_proba(X_race_selected)[:, 1]
        predictions = self.model.predict(X_race_selected)
        
        # çµæœã‚’ã¾ã¨ã‚
        results = race_data.copy()
        results['win_probability'] = probabilities
        results['predicted_winner'] = predictions
        results = results.sort_values('win_probability', ascending=False)
        
        return results
    
    def get_model_info(self):
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—"""
        return {
            'algorithm': best_model_name,
            'features': self.selected_features,
            'n_features': len(self.selected_features)
        }

# äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ã®ä½œæˆ
predictor = HorseRacingPredictor(
    model=final_model,
    scaler=scaler,
    feature_selector=selector,
    selected_features=selected_features
)

print("ç«¶é¦¬äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†")
print(f"ä½¿ç”¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {best_model_name}")
print(f"ç‰¹å¾´é‡æ•°: {len(selected_features)}")
```

## ã‚µãƒ³ãƒ—ãƒ«äºˆæƒ³ã®å®Ÿè¡Œ

```python
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰1ãƒ¬ãƒ¼ã‚¹ã‚’æŠ½å‡ºã—ã¦äºˆæƒ³ä¾‹ã‚’å®Ÿè¡Œ
sample_race_id = df_features['race_id'].unique()[0]
sample_race_data = df_features[df_features['race_id'] == sample_race_id].copy()

print(f"ãƒ¬ãƒ¼ã‚¹ID {sample_race_id} ã®äºˆæƒ³çµæœ:")
print("=" * 60)

# äºˆæƒ³å®Ÿè¡Œ
prediction_results = predictor.predict_race(sample_race_data)

# çµæœè¡¨ç¤º
display_columns = ['horse_id', 'popularity', 'odds', 'finish_position', 
                  'win_probability', 'predicted_winner']

for i, (_, horse) in enumerate(prediction_results[display_columns].iterrows(), 1):
    status = "â˜…" if horse['predicted_winner'] == 1 else " "
    actual = "â—¯" if horse['finish_position'] == 1 else " "
    print(f"{i:2d} {status} {actual} | "
          f"é¦¬ID: {horse['horse_id']:15} | "
          f"äººæ°—: {horse['popularity']:2.0f} | "
          f"ã‚ªãƒƒã‚º: {horse['odds']:5.1f} | "
          f"ç€é †: {horse['finish_position']:2.0f} | "
          f"å‹åˆ©ç¢ºç‡: {horse['win_probability']:.3f}")

print("\nè¨˜å·èª¬æ˜:")
print("â˜…: äºˆæƒ³1ä½, â—¯: å®Ÿéš›ã®å‹åˆ©é¦¬")

# çš„ä¸­ç¢ºèª
predicted_winner = prediction_results.iloc[0]
actual_winner = sample_race_data[sample_race_data['finish_position'] == 1].iloc[0]

if predicted_winner['horse_id'] == actual_winner['horse_id']:
    print("\nğŸ¯ äºˆæƒ³çš„ä¸­!")
else:
    print(f"\nâŒ äºˆæƒ³å¤–ã‚Œ (äºˆæƒ³: {predicted_winner['horse_id']}, å®Ÿéš›: {actual_winner['horse_id']})")
```

## ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿

```python
import joblib
from datetime import datetime

# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
model_info = {
    'model': final_model,
    'scaler': scaler,
    'feature_selector': selector,
    'selected_features': selected_features,
    'feature_columns': feature_columns,
    'algorithm': best_model_name,
    'test_accuracy': opt_test_acc if 'opt_test_acc' in locals() else test_acc,
    'test_auc': opt_test_auc if 'opt_test_auc' in locals() else test_auc,
    'created_at': datetime.now().isoformat()
}

# ä¿å­˜ï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯é©åˆ‡ãªãƒ‘ã‚¹ã‚’æŒ‡å®šï¼‰
# joblib.dump(model_info, 'horse_racing_model.pkl')
print("ãƒ¢ãƒ‡ãƒ«ä¿å­˜æº–å‚™å®Œäº†")

# èª­ã¿è¾¼ã¿ä¾‹
def load_model(filepath):
    """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    model_info = joblib.load(filepath)
    
    predictor = HorseRacingPredictor(
        model=model_info['model'],
        scaler=model_info['scaler'],
        feature_selector=model_info['feature_selector'],
        selected_features=model_info['selected_features']
    )
    
    return predictor, model_info

print("ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿æ©Ÿèƒ½æº–å‚™å®Œäº†")
```

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€scikit-learnã‚’ä½¿ç”¨ã—ãŸç«¶é¦¬äºˆæƒ³ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã®å…¨å·¥ç¨‹ã‚’è§£èª¬ã—ã¾ã—ãŸã€‚

### å­¦ç¿’ã—ãŸä¸»è¦ãªå†…å®¹ï¼š

1. **ãƒ‡ãƒ¼ã‚¿æº–å‚™**: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨EDA
2. **å‰å‡¦ç†**: æ¬ æå€¤å‡¦ç†ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
3. **ç‰¹å¾´é‡é¸æŠ**: çµ±è¨ˆçš„æ‰‹æ³•ã«ã‚ˆã‚‹é‡è¦ç‰¹å¾´é‡ã®æŠ½å‡º
4. **ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ**: 7ã¤ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½è©•ä¾¡
5. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: GridSearchCVã«ã‚ˆã‚‹æœ€é©åŒ–
6. **ãƒ¢ãƒ‡ãƒ«è©•ä¾¡**: ç²¾åº¦ã€AUCã€ROCæ›²ç·šã€æ··åŒè¡Œåˆ—
7. **ç‰¹å¾´é‡é‡è¦åº¦**: äºˆæƒ³ã«é‡è¦ãªè¦ç´ ã®åˆ†æ
8. **å®Ÿç”¨åŒ–**: äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

### scikit-learnã®ä¸»è¦ãªåˆ©ç‚¹ï¼š

- **çµ±ä¸€API**: ã™ã¹ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒ`fit()`, `predict()`ã§ä½¿ç”¨å¯èƒ½
- **è±Šå¯Œãªãƒ„ãƒ¼ãƒ«**: å‰å‡¦ç†ã€ç‰¹å¾´é‡é¸æŠã€è©•ä¾¡æŒ‡æ¨™ãŒå……å®Ÿ
- **å®Ÿç”¨çš„**: ç”£æ¥­ç•Œã§åºƒãä½¿ç”¨ã•ã‚Œã‚‹å®Ÿç¸¾
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: åˆå¿ƒè€…ã«ã‚‚ç†è§£ã—ã‚„ã™ã„èª¬æ˜

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼š

1. **ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿**: å®Ÿéš›ã®ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ´»ç”¨
2. **é«˜åº¦ãªç‰¹å¾´é‡**: è¡€çµ±æƒ…å ±ã€èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
3. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›
4. **æ·±å±¤å­¦ç¿’**: TensorFlow/PyTorchã¸ã®å±•é–‹
5. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæƒ³**: APIã‚µãƒ¼ãƒãƒ¼ã®æ§‹ç¯‰

scikit-learnã¯æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤ã‚’å­¦ã¶æœ€é©ãªãƒ„ãƒ¼ãƒ«ã§ã™ã€‚æœ¬è¨˜äº‹ã§å­¦ã‚“ã æŠ€è¡“ã‚’åŸºã«ã€ã‚ˆã‚Šé«˜åº¦ãªäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«æŒ‘æˆ¦ã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ä¸–ç•Œã¸ã®ç¬¬ä¸€æ­©ã¨ã—ã¦ã€scikit-learnã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹ã“ã¨ã§ã€æ§˜ã€…ãªå•é¡Œã«å¿œç”¨ã§ãã‚‹æ±ç”¨çš„ãªã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚