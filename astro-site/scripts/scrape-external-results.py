#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤–éƒ¨ã‚µã‚¤ãƒˆå®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿è‡ªå‹•å–ã‚Šè¾¼ã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
apolon.keibanahibi.com ã‹ã‚‰12å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦archiveResults.jsonå½¢å¼ã«å¤‰æ›
"""

import re
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Any

# å¤–éƒ¨ã‚µã‚¤ãƒˆãƒ™ãƒ¼ã‚¹URL
BASE_URL = "https://apolon.keibanahibi.com/index.php"

def generate_month_urls(start_year: int = 2013, start_month: int = 9) -> List[str]:
    """
    2013å¹´9æœˆã€œç¾åœ¨ã¾ã§ã®æœˆåˆ¥URLç”Ÿæˆ
    """
    urls = []
    current = datetime.now()

    year = start_year
    month = start_month

    while True:
        # URLç”Ÿæˆ: ?kaiinkekka_YYYYMM
        url = f"{BASE_URL}?kaiinkekka_{year}{month:02d}"
        urls.append((url, year, month))

        # æ¬¡ã®æœˆã¸
        month += 1
        if month > 12:
            month = 1
            year += 1

        # ç¾åœ¨æœˆã‚’è¶…ãˆãŸã‚‰çµ‚äº†
        if year > current.year or (year == current.year and month > current.month):
            break

    return urls

def parse_race_result_page(html: str, year: int, month: int) -> List[Dict[str, Any]]:
    """
    1ãƒšãƒ¼ã‚¸åˆ†ã®HTMLè§£æã—ã¦ãƒ¬ãƒ¼ã‚¹çµæœã‚’æŠ½å‡º

    æœŸå¾…ã•ã‚Œã‚‹æ§‹é€ :
    8/31èˆ¹æ©‹ç«¶é¦¬äºˆæƒ³ æœ‰æ–™ç‰ˆ çµæœ
    1R 12-8é¦¬å˜ 600å†† çš„ä¸­ï¼
    2R 1-9é¦¬å˜ 3.990å†† çš„ä¸­ï¼
    """
    soup = BeautifulSoup(html, 'html.parser')
    results = []

    # æ—¥ä»˜è¦‹å‡ºã—ã‚’æ¢ç´¢ï¼ˆä¾‹: "8/31èˆ¹æ©‹ç«¶é¦¬äºˆæƒ³ æœ‰æ–™ç‰ˆ"ï¼‰
    date_pattern = re.compile(r'(\d{1,2})/(\d{1,2})\s*(\S+)ç«¶é¦¬')

    # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    page_text = soup.get_text()

    # æ—¥ä»˜ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«åˆ†å‰²
    current_date = None
    current_venue = None

    for line in page_text.split('\n'):
        line = line.strip()
        if not line:
            continue

        # æ—¥ä»˜ãƒ»ç«¶é¦¬å ´æ¤œå‡º
        date_match = date_pattern.search(line)
        if date_match:
            m, d, venue = date_match.groups()
            current_date = f"{year}-{int(m):02d}-{int(d):02d}"
            current_venue = venue
            continue

        # ãƒ¬ãƒ¼ã‚¹çµæœæ¤œå‡ºï¼ˆä¾‹: "1R 12-8é¦¬å˜ 600å†† çš„ä¸­ï¼"ï¼‰
        race_pattern = re.compile(r'(\d{1,2})R\s+(\d+)-(\d+)é¦¬å˜\s+([\d,\.]+)å††\s+çš„ä¸­')
        race_match = race_pattern.search(line)

        if race_match and current_date and current_venue:
            race_num, horse1, horse2, payout_str = race_match.groups()

            # é…å½“é‡‘é¡ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚«ãƒ³ãƒãƒ»ãƒ”ãƒªã‚ªãƒ‰é™¤å»ï¼‰
            payout = int(payout_str.replace(',', '').replace('.', ''))

            results.append({
                'date': current_date,
                'venue': current_venue,
                'raceNumber': int(race_num),
                'umatan': f"{horse1}-{horse2}",
                'payout': payout,
                'hit': True
            })

    return results

def convert_to_archive_format(scraped_data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’archiveResults.jsonå½¢å¼ã«å¤‰æ›

    archiveResults.jsonå½¢å¼:
    {
      "2025": {
        "10": [
          {
            "date": "2025-10-16",
            "venue": "å·å´",
            "totalRaces": 12,
            "hitRaces": 7,
            "hitRate": "58%",
            "totalPayout": 13040,
            "recoveryRate": "124%",
            "races": [...]
          }
        ]
      }
    }
    """
    archive = {}

    # æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    daily_groups = {}
    for race in scraped_data:
        date = race['date']
        if date not in daily_groups:
            daily_groups[date] = []
        daily_groups[date].append(race)

    # archiveResults.jsonå½¢å¼ã«å¤‰æ›
    for date, races in daily_groups.items():
        year, month, day = date.split('-')

        if year not in archive:
            archive[year] = {}
        if month not in archive[year]:
            archive[year][month] = []

        # æ—¥åˆ¥ã‚µãƒãƒªãƒ¼ä½œæˆ
        venue = races[0]['venue'] if races else 'ä¸æ˜'
        hit_races = len(races)  # çš„ä¸­ãƒ¬ãƒ¼ã‚¹ã®ã¿ãªã®ã§hit_races = len(races)
        total_payout = sum(r['payout'] for r in races)

        # âš ï¸ è­¦å‘Šï¼šä¸çš„ä¸­ãƒ¬ãƒ¼ã‚¹æ•°ãŒä¸æ˜ãªã®ã§ã€totalRacesã¯æ¨æ¸¬
        # å—é–¢ç«¶é¦¬ã¯é€šå¸¸12R or 11R or 10R or 8R
        # æœ€å¤§ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‹ã‚‰æ¨æ¸¬
        max_race_num = max(r['raceNumber'] for r in races)
        total_races = max_race_num  # ä¿å®ˆçš„æ¨æ¸¬

        hit_rate_value = (hit_races / total_races * 100) if total_races > 0 else 0
        hit_rate = f"{hit_rate_value:.0f}%"

        # âš ï¸ è­¦å‘Šï¼šè³¼å…¥ç‚¹æ•°æƒ…å ±ãŒãªã„ãŸã‚å›åç‡è¨ˆç®—ä¸å¯
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šé¦¬å˜1ç‚¹ = 100å††ã¨ã—ã¦è¨ˆç®—
        total_bet = hit_races * 100  # å…¨çš„ä¸­ãƒ¬ãƒ¼ã‚¹Ã—100å††ï¼ˆé¦¬å˜1ç‚¹æƒ³å®šï¼‰
        recovery_rate_value = (total_payout / total_bet * 100) if total_bet > 0 else 0
        recovery_rate = f"{recovery_rate_value:.0f}%"

        # ãƒ¬ãƒ¼ã‚¹è©³ç´°ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        race_details = []
        for r in races:
            race_details.append({
                'raceNumber': r['raceNumber'],
                'raceName': f"{r['raceNumber']}R",  # ãƒ¬ãƒ¼ã‚¹åæƒ…å ±ãªã—
                'strategy': 'ãƒãƒ©ãƒ³ã‚¹å‹',  # æˆ¦ç•¥æƒ…å ±ãªã—ãƒ»é¦¬å˜ãªã®ã§ãƒãƒ©ãƒ³ã‚¹å‹ã¨æ¨æ¸¬
                'betType': 'é¦¬å˜',
                'betDetails': r['umatan'],
                'result': 'âœ… çš„ä¸­',
                'payout': f"Â¥{r['payout']:,}"
            })

        # æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        archive[year][month].append({
            'date': date,
            'venue': venue,
            'totalRaces': total_races,
            'hitRaces': hit_races,
            'hitRate': hit_rate,
            'totalPayout': total_payout,
            'recoveryRate': recovery_rate,
            'races': race_details
        })

    return archive

def scrape_all_results(start_year: int = 2013, start_month: int = 9) -> Dict[str, Any]:
    """
    å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    """
    print("ğŸ” å¤–éƒ¨ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
    print(f"ğŸ“… å¯¾è±¡æœŸé–“: {start_year}å¹´{start_month}æœˆã€œç¾åœ¨")

    urls = generate_month_urls(start_year, start_month)
    print(f"ğŸ“Š ç·URLæ•°: {len(urls)}ä»¶\n")

    all_results = []
    success_count = 0
    error_count = 0

    for url, year, month in urls:
        try:
            print(f"â³ å–å¾—ä¸­: {year}å¹´{month:02d}æœˆ... ", end='', flush=True)
            response = requests.get(url, timeout=10)

            if response.status_code == 404:
                print("âŒ 404 Not Found")
                error_count += 1
                continue

            response.raise_for_status()

            # HTMLè§£æ
            results = parse_race_result_page(response.text, year, month)

            if results:
                all_results.extend(results)
                print(f"âœ… {len(results)}ãƒ¬ãƒ¼ã‚¹å–å¾—")
                success_count += 1
            else:
                print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—")

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            error_count += 1

    print(f"\nğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")
    print(f"âœ… æˆåŠŸ: {success_count}ãƒ¶æœˆ")
    print(f"âŒ å¤±æ•—: {error_count}ãƒ¶æœˆ")
    print(f"ğŸ‡ ç·ãƒ¬ãƒ¼ã‚¹æ•°: {len(all_results)}ãƒ¬ãƒ¼ã‚¹\n")

    # archiveResults.jsonå½¢å¼ã«å¤‰æ›
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿å¤‰æ›ä¸­...")
    archive = convert_to_archive_format(all_results)
    print("âœ… å¤‰æ›å®Œäº†\n")

    return archive

def merge_with_existing_archive(new_data: Dict[str, Any], existing_file: str) -> Dict[str, Any]:
    """
    æ—¢å­˜ã®archiveResults.jsonã¨ãƒãƒ¼ã‚¸ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿å„ªå…ˆï¼‰
    """
    try:
        with open(existing_file, 'r', encoding='utf-8') as f:
            existing = json.load(f)
        print(f"ğŸ“‚ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {existing_file}")
    except FileNotFoundError:
        print(f"âš ï¸ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ã€æ–°è¦ä½œæˆã—ã¾ã™")
        existing = {}

    # ãƒãƒ¼ã‚¸ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿å„ªå…ˆï¼‰
    merged = existing.copy()

    for year, months in new_data.items():
        if year not in merged:
            merged[year] = {}

        for month, days in months.items():
            if month not in merged[year]:
                merged[year][month] = []

            # æ—¢å­˜ã®æ—¥ä»˜ã‚’å–å¾—
            existing_dates = {d['date'] for d in merged[year][month]}

            # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿è¿½åŠ ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯ä¸Šæ›¸ãã—ãªã„ï¼‰
            for day_data in days:
                if day_data['date'] not in existing_dates:
                    merged[year][month].append(day_data)

    return merged

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("=" * 60)
    print("ğŸ‡ å¤–éƒ¨ã‚µã‚¤ãƒˆå®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿è‡ªå‹•å–ã‚Šè¾¼ã¿ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    print()

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    scraped_data = scrape_all_results(start_year=2013, start_month=9)

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒãƒ¼ã‚¸
    archive_file = "../src/data/archiveResults.json"
    merged_data = merge_with_existing_archive(scraped_data, archive_file)

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    output_file = "scraped_archive_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {output_file}")
    print()
    print("=" * 60)
    print("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("=" * 60)
    print()
    print("ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print(f"1. {output_file} ã®å†…å®¹ã‚’ç¢ºèª")
    print("2. å•é¡Œãªã‘ã‚Œã° src/data/archiveResults.json ã«ã‚³ãƒ”ãƒ¼")
    print("3. git add, commit, push ã§æœ¬ç•ªåæ˜ ")

if __name__ == "__main__":
    main()
